{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = keras.utils.get_file('spa-eng.zip', origin='https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\salim\\\\.keras\\\\datasets\\\\spa-eng_extracted'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\salim\\\\.keras\\\\datasets\\\\spa-eng_extracted\\\\spa-eng\\\\spa.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), 'spa-eng_extracted', 'spa-eng', 'spa.txt')\n",
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام\n",
      "بلال\n",
      "resume\n"
     ]
    }
   ],
   "source": [
    "print(unicode_to_ascii(\"سلام\"))     \n",
    "print(unicode_to_ascii(\"بَلالِ\"))   \n",
    "print(unicode_to_ascii(\"résumé\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_senetence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.~,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.~,]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    w = '<strat> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<strat> i have some friends to help . <end>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = \"I have some friends to help.\"\n",
    "preprocess_senetence(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<strat> aun no he dicho nada . <end>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_sentence = \"Aún no he dicho nada.\"\n",
    "preprocess_senetence(sp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_senetence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    # print(word_pairs)\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x0000021555073B00>\n"
     ]
    }
   ],
   "source": [
    "print(create_dataset(path_to_file, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, target_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, target_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer = load_dataset(path_to_file, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  97,   3, ...,   0,   0,   0],\n",
       "       [  1, 179,   3, ...,   0,   0,   0],\n",
       "       [  1, 466,   3, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,   7,  10, ...,   0,   0,   0],\n",
       "       [  1,   7,  10, ...,   0,   0,   0],\n",
       "       [  1,   7,  10, ...,   0,   0,   0]],\n",
       "      shape=(20000, 16), dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(t, ' .... ', lang.index_word[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ....  <strat>\n",
      "97  ....  ve\n",
      "3  ....  .\n",
      "2  ....  <end>\n"
     ]
    }
   ],
   "source": [
    "convert(input_lang_tokenizer, input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 97,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<strat>': 1,\n",
       " '<end>': 2,\n",
       " '.': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " '?': 6,\n",
       " 'tom': 7,\n",
       " 'is': 8,\n",
       " 'it': 9,\n",
       " 's': 10,\n",
       " 'a': 11,\n",
       " 'he': 12,\n",
       " 't': 13,\n",
       " 'the': 14,\n",
       " 'we': 15,\n",
       " 'm': 16,\n",
       " 'me': 17,\n",
       " 're': 18,\n",
       " 'that': 19,\n",
       " 'this': 20,\n",
       " 'to': 21,\n",
       " 'do': 22,\n",
       " 'are': 23,\n",
       " 'can': 24,\n",
       " 'my': 25,\n",
       " 'they': 26,\n",
       " 'was': 27,\n",
       " 'she': 28,\n",
       " 'don': 29,\n",
       " 'have': 30,\n",
       " 'your': 31,\n",
       " 'go': 32,\n",
       " 'what': 33,\n",
       " 'in': 34,\n",
       " 'not': 35,\n",
       " 'll': 36,\n",
       " 'like': 37,\n",
       " 'here': 38,\n",
       " 'on': 39,\n",
       " 'him': 40,\n",
       " 'let': 41,\n",
       " 'be': 42,\n",
       " 'did': 43,\n",
       " 'know': 44,\n",
       " 'come': 45,\n",
       " 'up': 46,\n",
       " 'am': 47,\n",
       " 'want': 48,\n",
       " 'how': 49,\n",
       " ',': 50,\n",
       " 'mary': 51,\n",
       " 'get': 52,\n",
       " 'who': 53,\n",
       " 'very': 54,\n",
       " 'now': 55,\n",
       " 'need': 56,\n",
       " 'has': 57,\n",
       " 'please': 58,\n",
       " 'no': 59,\n",
       " 'there': 60,\n",
       " 'help': 61,\n",
       " 'her': 62,\n",
       " 'love': 63,\n",
       " 'at': 64,\n",
       " 'see': 65,\n",
       " 'just': 66,\n",
       " 'out': 67,\n",
       " 'his': 68,\n",
       " 've': 69,\n",
       " 'got': 70,\n",
       " 'for': 71,\n",
       " 'where': 72,\n",
       " 'look': 73,\n",
       " 'stop': 74,\n",
       " 'one': 75,\n",
       " 'us': 76,\n",
       " 'good': 77,\n",
       " 'car': 78,\n",
       " 'too': 79,\n",
       " 'so': 80,\n",
       " 'all': 81,\n",
       " 'why': 82,\n",
       " 'will': 83,\n",
       " 'an': 84,\n",
       " 'home': 85,\n",
       " 'of': 86,\n",
       " 'with': 87,\n",
       " 'give': 88,\n",
       " 'back': 89,\n",
       " 'were': 90,\n",
       " 'keep': 91,\n",
       " 'take': 92,\n",
       " 'dog': 93,\n",
       " 'saw': 94,\n",
       " 'didn': 95,\n",
       " 'isn': 96,\n",
       " 'may': 97,\n",
       " 'happy': 98,\n",
       " 'stay': 99,\n",
       " 'won': 100,\n",
       " 'work': 101,\n",
       " 'hate': 102,\n",
       " 'must': 103,\n",
       " 'wait': 104,\n",
       " 'leave': 105,\n",
       " 'again': 106,\n",
       " 'likes': 107,\n",
       " 'down': 108,\n",
       " 'feel': 109,\n",
       " 'book': 110,\n",
       " 'try': 111,\n",
       " 'made': 112,\n",
       " 'eat': 113,\n",
       " 'right': 114,\n",
       " 'them': 115,\n",
       " 'still': 116,\n",
       " 'had': 117,\n",
       " 'time': 118,\n",
       " 'going': 119,\n",
       " 'does': 120,\n",
       " 'money': 121,\n",
       " 'call': 122,\n",
       " 'say': 123,\n",
       " 'lost': 124,\n",
       " 'came': 125,\n",
       " 'tell': 126,\n",
       " 'went': 127,\n",
       " 'well': 128,\n",
       " 'today': 129,\n",
       " 'old': 130,\n",
       " 'busy': 131,\n",
       " 'looks': 132,\n",
       " 'ask': 133,\n",
       " 'away': 134,\n",
       " 'loves': 135,\n",
       " 'job': 136,\n",
       " 'man': 137,\n",
       " 'bad': 138,\n",
       " 'everyone': 139,\n",
       " 'never': 140,\n",
       " 'some': 141,\n",
       " 'over': 142,\n",
       " 'pay': 143,\n",
       " 'mine': 144,\n",
       " 'from': 145,\n",
       " 'ready': 146,\n",
       " 'alone': 147,\n",
       " 'read': 148,\n",
       " 'wrong': 149,\n",
       " 'room': 150,\n",
       " 'live': 151,\n",
       " 'angry': 152,\n",
       " 'tired': 153,\n",
       " 'talk': 154,\n",
       " 'more': 155,\n",
       " 'make': 156,\n",
       " 'could': 157,\n",
       " 'about': 158,\n",
       " 'nice': 159,\n",
       " 'nobody': 160,\n",
       " 'off': 161,\n",
       " 'french': 162,\n",
       " 'house': 163,\n",
       " 'hurt': 164,\n",
       " 'speak': 165,\n",
       " 'watch': 166,\n",
       " 'should': 167,\n",
       " 'left': 168,\n",
       " 'cold': 169,\n",
       " 'big': 170,\n",
       " 'late': 171,\n",
       " 'play': 172,\n",
       " 'new': 173,\n",
       " 'true': 174,\n",
       " 'and': 175,\n",
       " 'drink': 176,\n",
       " 'life': 177,\n",
       " 'our': 178,\n",
       " 'drunk': 179,\n",
       " 'lot': 180,\n",
       " 'open': 181,\n",
       " 'said': 182,\n",
       " 'these': 183,\n",
       " 'wasn': 184,\n",
       " 'boston': 185,\n",
       " 'way': 186,\n",
       " 'fast': 187,\n",
       " 'boy': 188,\n",
       " 'nothing': 189,\n",
       " 'turn': 190,\n",
       " 'understand': 191,\n",
       " 'ok': 192,\n",
       " 'hungry': 193,\n",
       " 'wants': 194,\n",
       " 'when': 195,\n",
       " 'really': 196,\n",
       " 'ate': 197,\n",
       " 'hurry': 198,\n",
       " 'died': 199,\n",
       " 'hot': 200,\n",
       " 'felt': 201,\n",
       " 'think': 202,\n",
       " 'name': 203,\n",
       " 'find': 204,\n",
       " 'sick': 205,\n",
       " 'yours': 206,\n",
       " 'everybody': 207,\n",
       " 'sit': 208,\n",
       " 'gave': 209,\n",
       " 'smart': 210,\n",
       " 'answer': 211,\n",
       " 'broke': 212,\n",
       " 'aren': 213,\n",
       " 'been': 214,\n",
       " 'hold': 215,\n",
       " 'miss': 216,\n",
       " 'stupid': 217,\n",
       " 'fun': 218,\n",
       " 'done': 219,\n",
       " 'sleep': 220,\n",
       " 'everything': 221,\n",
       " 'already': 222,\n",
       " 'listen': 223,\n",
       " 'fine': 224,\n",
       " 'bed': 225,\n",
       " 'first': 226,\n",
       " 'much': 227,\n",
       " 'sing': 228,\n",
       " 'loved': 229,\n",
       " 'coming': 230,\n",
       " 'great': 231,\n",
       " 'bought': 232,\n",
       " 'two': 233,\n",
       " 'father': 234,\n",
       " 'long': 235,\n",
       " 'cry': 236,\n",
       " 'married': 237,\n",
       " 'cat': 238,\n",
       " 'best': 239,\n",
       " 'tv': 240,\n",
       " 'hear': 241,\n",
       " 'friend': 242,\n",
       " 'ran': 243,\n",
       " 'stand': 244,\n",
       " 'crazy': 245,\n",
       " 'knows': 246,\n",
       " 'mad': 247,\n",
       " 'swim': 248,\n",
       " 'close': 249,\n",
       " 'idea': 250,\n",
       " 'easy': 251,\n",
       " 'rich': 252,\n",
       " 'hard': 253,\n",
       " 'walk': 254,\n",
       " 'start': 255,\n",
       " 'kill': 256,\n",
       " 'run': 257,\n",
       " 'put': 258,\n",
       " 'knew': 259,\n",
       " 'both': 260,\n",
       " 'doctor': 261,\n",
       " 'needs': 262,\n",
       " 'something': 263,\n",
       " 'almost': 264,\n",
       " 'wife': 265,\n",
       " 'only': 266,\n",
       " 'hair': 267,\n",
       " 'show': 268,\n",
       " 'by': 269,\n",
       " 'sad': 270,\n",
       " 'move': 271,\n",
       " 'remember': 272,\n",
       " 'dead': 273,\n",
       " 'gone': 274,\n",
       " 'crying': 275,\n",
       " 'drive': 276,\n",
       " 'free': 277,\n",
       " 'write': 278,\n",
       " 'school': 279,\n",
       " 'pretty': 280,\n",
       " 'those': 281,\n",
       " 'doesn': 282,\n",
       " 'trust': 283,\n",
       " 'tall': 284,\n",
       " 'd': 285,\n",
       " 'better': 286,\n",
       " 'friends': 287,\n",
       " 'fat': 288,\n",
       " 'sat': 289,\n",
       " 'bus': 290,\n",
       " 'bit': 291,\n",
       " 'heard': 292,\n",
       " 'water': 293,\n",
       " 'lie': 294,\n",
       " 'early': 295,\n",
       " 'called': 296,\n",
       " 'die': 297,\n",
       " 'hand': 298,\n",
       " 'met': 299,\n",
       " 'yourself': 300,\n",
       " 'bag': 301,\n",
       " 'teacher': 302,\n",
       " 'eyes': 303,\n",
       " 'bring': 304,\n",
       " 'lying': 305,\n",
       " 'yes': 306,\n",
       " 'day': 307,\n",
       " 'door': 308,\n",
       " 'believe': 309,\n",
       " 'doing': 310,\n",
       " 'enemy': 311,\n",
       " 'mean': 312,\n",
       " 'soon': 313,\n",
       " 'dogs': 314,\n",
       " 'wine': 315,\n",
       " 'buy': 316,\n",
       " 'as': 317,\n",
       " 'young': 318,\n",
       " 'monday': 319,\n",
       " 'singing': 320,\n",
       " 'red': 321,\n",
       " 'helped': 322,\n",
       " 'looked': 323,\n",
       " 'afraid': 324,\n",
       " 'someone': 325,\n",
       " 'quit': 326,\n",
       " 'guys': 327,\n",
       " 'use': 328,\n",
       " 'follow': 329,\n",
       " 'food': 330,\n",
       " 'found': 331,\n",
       " 'lucky': 332,\n",
       " 'key': 333,\n",
       " 'lunch': 334,\n",
       " 'tomorrow': 335,\n",
       " 'coffee': 336,\n",
       " 'kind': 337,\n",
       " 'hit': 338,\n",
       " 'safe': 339,\n",
       " 'forget': 340,\n",
       " 'study': 341,\n",
       " 'later': 342,\n",
       " 'whose': 343,\n",
       " 'seen': 344,\n",
       " 'enough': 345,\n",
       " 'running': 346,\n",
       " 'small': 347,\n",
       " 'son': 348,\n",
       " 'always': 349,\n",
       " 'enjoy': 350,\n",
       " 'might': 351,\n",
       " 'hope': 352,\n",
       " 'awake': 353,\n",
       " 'shot': 354,\n",
       " 'told': 355,\n",
       " 'fish': 356,\n",
       " 'arrived': 357,\n",
       " 'hat': 358,\n",
       " 'break': 359,\n",
       " 'short': 360,\n",
       " 'took': 361,\n",
       " 'quiet': 362,\n",
       " 'next': 363,\n",
       " 'works': 364,\n",
       " 'eating': 365,\n",
       " 'night': 366,\n",
       " 'asked': 367,\n",
       " 'cats': 368,\n",
       " 'milk': 369,\n",
       " 'seems': 370,\n",
       " 'dream': 371,\n",
       " 'dinner': 372,\n",
       " 'watching': 373,\n",
       " 'mother': 374,\n",
       " 'began': 375,\n",
       " 'thanks': 376,\n",
       " 'cool': 377,\n",
       " 'care': 378,\n",
       " 'real': 379,\n",
       " 'hurts': 380,\n",
       " 'cook': 381,\n",
       " 'check': 382,\n",
       " 'beer': 383,\n",
       " 'smoke': 384,\n",
       " 'hide': 385,\n",
       " 'fired': 386,\n",
       " 'rain': 387,\n",
       " 'send': 388,\n",
       " 'changed': 389,\n",
       " 'myself': 390,\n",
       " 'talking': 391,\n",
       " 'fault': 392,\n",
       " 'which': 393,\n",
       " 'hands': 394,\n",
       " 'yet': 395,\n",
       " 'brave': 396,\n",
       " 'full': 397,\n",
       " 'around': 398,\n",
       " 'liked': 399,\n",
       " 'problem': 400,\n",
       " 'win': 401,\n",
       " 'seat': 402,\n",
       " 'joking': 403,\n",
       " 'broken': 404,\n",
       " 'secret': 405,\n",
       " 'rest': 406,\n",
       " 'books': 407,\n",
       " 'hates': 408,\n",
       " 'english': 409,\n",
       " 'owe': 410,\n",
       " 'girl': 411,\n",
       " 'family': 412,\n",
       " 'would': 413,\n",
       " 'fell': 414,\n",
       " 'shy': 415,\n",
       " 'sorry': 416,\n",
       " 'along': 417,\n",
       " 'men': 418,\n",
       " 'kids': 419,\n",
       " 'naive': 420,\n",
       " 'reading': 421,\n",
       " 'working': 422,\n",
       " 'joke': 423,\n",
       " 'clean': 424,\n",
       " 'sure': 425,\n",
       " 'word': 426,\n",
       " 'little': 427,\n",
       " 'lied': 428,\n",
       " 'calm': 429,\n",
       " 'forgot': 430,\n",
       " 'warm': 431,\n",
       " 'patient': 432,\n",
       " 'missed': 433,\n",
       " 'begin': 434,\n",
       " 'snow': 435,\n",
       " 'needed': 436,\n",
       " 'woman': 437,\n",
       " 'lawyer': 438,\n",
       " 'beautiful': 439,\n",
       " 'happen': 440,\n",
       " 'kept': 441,\n",
       " 'lives': 442,\n",
       " 'child': 443,\n",
       " 'person': 444,\n",
       " 'tried': 445,\n",
       " 'shut': 446,\n",
       " 'fix': 447,\n",
       " 'failed': 448,\n",
       " 'hey': 449,\n",
       " 'promised': 450,\n",
       " 'anybody': 451,\n",
       " 'change': 452,\n",
       " 'funny': 453,\n",
       " 'once': 454,\n",
       " 'pen': 455,\n",
       " 'drank': 456,\n",
       " 'things': 457,\n",
       " 'plan': 458,\n",
       " 'own': 459,\n",
       " 'insane': 460,\n",
       " 'truth': 461,\n",
       " 'game': 462,\n",
       " 'born': 463,\n",
       " 'hang': 464,\n",
       " 'wake': 465,\n",
       " 'smiled': 466,\n",
       " 'cut': 467,\n",
       " 'boring': 468,\n",
       " 'empty': 469,\n",
       " 'kidding': 470,\n",
       " 'smell': 471,\n",
       " 'meet': 472,\n",
       " 'happened': 473,\n",
       " 'wish': 474,\n",
       " 'blue': 475,\n",
       " 'outside': 476,\n",
       " 'studying': 477,\n",
       " 'useless': 478,\n",
       " 'box': 479,\n",
       " 'somebody': 480,\n",
       " 'clever': 481,\n",
       " 'shoes': 482,\n",
       " 'fire': 483,\n",
       " 'moved': 484,\n",
       " 'perfect': 485,\n",
       " 'kiss': 486,\n",
       " 'laughed': 487,\n",
       " 'blind': 488,\n",
       " 'far': 489,\n",
       " 'worked': 490,\n",
       " 'dark': 491,\n",
       " 'thank': 492,\n",
       " 'anyone': 493,\n",
       " 'meat': 494,\n",
       " 'hiding': 495,\n",
       " 'alive': 496,\n",
       " 'white': 497,\n",
       " 'dance': 498,\n",
       " 'cheated': 499,\n",
       " 'confused': 500,\n",
       " 'black': 501,\n",
       " 'sweet': 502,\n",
       " 'cake': 503,\n",
       " 'japanese': 504,\n",
       " 'people': 505,\n",
       " 'raining': 506,\n",
       " 'minute': 507,\n",
       " 'keys': 508,\n",
       " 'breath': 509,\n",
       " 'idiot': 510,\n",
       " 'phone': 511,\n",
       " 'betrayed': 512,\n",
       " 'story': 513,\n",
       " 'hi': 514,\n",
       " 'agree': 515,\n",
       " 'cute': 516,\n",
       " 'save': 517,\n",
       " 'fly': 518,\n",
       " 'slowly': 519,\n",
       " 'tea': 520,\n",
       " 'single': 521,\n",
       " 'fight': 522,\n",
       " 'blame': 523,\n",
       " 'trapped': 524,\n",
       " 'party': 525,\n",
       " 'face': 526,\n",
       " 'rules': 527,\n",
       " 'music': 528,\n",
       " 'lonely': 529,\n",
       " 'last': 530,\n",
       " 'stopped': 531,\n",
       " 'explain': 532,\n",
       " 'asleep': 533,\n",
       " 'six': 534,\n",
       " 'guy': 535,\n",
       " 'brother': 536,\n",
       " 'any': 537,\n",
       " 'place': 538,\n",
       " 'started': 539,\n",
       " 'beat': 540,\n",
       " 'deep': 541,\n",
       " 'talked': 542,\n",
       " 'warn': 543,\n",
       " 'dying': 544,\n",
       " 'sign': 545,\n",
       " 'ice': 546,\n",
       " 'wrote': 547,\n",
       " 'count': 548,\n",
       " 'finish': 549,\n",
       " 'dumb': 550,\n",
       " 'often': 551,\n",
       " 'naked': 552,\n",
       " 'waiting': 553,\n",
       " 'anything': 554,\n",
       " 'became': 555,\n",
       " 'bath': 556,\n",
       " 'sang': 557,\n",
       " 'light': 558,\n",
       " 'listening': 559,\n",
       " 'touch': 560,\n",
       " 'head': 561,\n",
       " 'seem': 562,\n",
       " 'glasses': 563,\n",
       " 'sounds': 564,\n",
       " 'tie': 565,\n",
       " 'into': 566,\n",
       " 'couldn': 567,\n",
       " 'mouth': 568,\n",
       " 'being': 569,\n",
       " 'speaks': 570,\n",
       " 'slept': 571,\n",
       " 'welcome': 572,\n",
       " 'bald': 573,\n",
       " 'weak': 574,\n",
       " 'helps': 575,\n",
       " 'paid': 576,\n",
       " 'after': 577,\n",
       " 'inside': 578,\n",
       " 'cried': 579,\n",
       " 'forgive': 580,\n",
       " 'god': 581,\n",
       " 'survived': 582,\n",
       " 'moving': 583,\n",
       " 'sleepy': 584,\n",
       " 'green': 585,\n",
       " 'curious': 586,\n",
       " 'excited': 587,\n",
       " 'doll': 588,\n",
       " 'pain': 589,\n",
       " 'another': 590,\n",
       " 'arm': 591,\n",
       " 'losing': 592,\n",
       " 'fill': 593,\n",
       " 'gun': 594,\n",
       " 'comes': 595,\n",
       " 'tennis': 596,\n",
       " 'week': 597,\n",
       " 'advice': 598,\n",
       " 'important': 599,\n",
       " 'horse': 600,\n",
       " 'dry': 601,\n",
       " 'learn': 602,\n",
       " 'turned': 603,\n",
       " 'fishing': 604,\n",
       " 'canadian': 605,\n",
       " 'summer': 606,\n",
       " 'choice': 607,\n",
       " 'quite': 608,\n",
       " 'war': 609,\n",
       " 'cannot': 610,\n",
       " 'paper': 611,\n",
       " 'three': 612,\n",
       " 'smile': 613,\n",
       " 'fair': 614,\n",
       " 'catch': 615,\n",
       " 'weird': 616,\n",
       " 'ours': 617,\n",
       " 'careful': 618,\n",
       " 'serious': 619,\n",
       " 'hero': 620,\n",
       " 'liar': 621,\n",
       " 'normal': 622,\n",
       " 'trying': 623,\n",
       " 'cheese': 624,\n",
       " 'walked': 625,\n",
       " 'strong': 626,\n",
       " 'song': 627,\n",
       " 'shall': 628,\n",
       " 'sleeping': 629,\n",
       " 'horses': 630,\n",
       " 'student': 631,\n",
       " 'forgetful': 632,\n",
       " 'together': 633,\n",
       " 'second': 634,\n",
       " 'luck': 635,\n",
       " 'raise': 636,\n",
       " 'mistake': 637,\n",
       " 'clock': 638,\n",
       " 'trouble': 639,\n",
       " 'end': 640,\n",
       " 'getting': 641,\n",
       " 'noise': 642,\n",
       " 'maybe': 643,\n",
       " 'brown': 644,\n",
       " 'himself': 645,\n",
       " 'shirt': 646,\n",
       " 'okay': 647,\n",
       " 'poor': 648,\n",
       " 'doubt': 649,\n",
       " 'laugh': 650,\n",
       " 'strange': 651,\n",
       " 'news': 652,\n",
       " 'baffled': 653,\n",
       " 'cooking': 654,\n",
       " 'nervous': 655,\n",
       " 'worried': 656,\n",
       " 'mess': 657,\n",
       " 'invited': 658,\n",
       " 'quickly': 659,\n",
       " 'apples': 660,\n",
       " 'women': 661,\n",
       " 'thrilled': 662,\n",
       " 'writing': 663,\n",
       " 'closed': 664,\n",
       " 'boss': 665,\n",
       " 'thirsty': 666,\n",
       " 'deal': 667,\n",
       " 'satisfied': 668,\n",
       " 'worse': 669,\n",
       " 'duty': 670,\n",
       " 'simple': 671,\n",
       " 'cup': 672,\n",
       " 'also': 673,\n",
       " 'truck': 674,\n",
       " 'if': 675,\n",
       " 'throw': 676,\n",
       " 'killed': 677,\n",
       " 'apple': 678,\n",
       " 'letter': 679,\n",
       " 'sister': 680,\n",
       " 'teeth': 681,\n",
       " 'bicycle': 682,\n",
       " 'easily': 683,\n",
       " 'jump': 684,\n",
       " 'join': 685,\n",
       " 'wash': 686,\n",
       " 'waited': 687,\n",
       " 'ignore': 688,\n",
       " 'walks': 689,\n",
       " 'cruel': 690,\n",
       " 'bored': 691,\n",
       " 'town': 692,\n",
       " 'bread': 693,\n",
       " 'soup': 694,\n",
       " 'gas': 695,\n",
       " 'saved': 696,\n",
       " 'many': 697,\n",
       " 'kissed': 698,\n",
       " 'team': 699,\n",
       " 'bear': 700,\n",
       " 'trap': 701,\n",
       " 'snowing': 702,\n",
       " 'smoking': 703,\n",
       " 'pale': 704,\n",
       " 'dad': 705,\n",
       " 'nose': 706,\n",
       " 'kid': 707,\n",
       " 'trusted': 708,\n",
       " 'watched': 709,\n",
       " 'robbed': 710,\n",
       " 'favor': 711,\n",
       " 'feeling': 712,\n",
       " 'undressing': 713,\n",
       " 'used': 714,\n",
       " 'cheap': 715,\n",
       " 'wonderful': 716,\n",
       " 'heart': 717,\n",
       " 'blew': 718,\n",
       " 'swimming': 719,\n",
       " 'paris': 720,\n",
       " 'side': 721,\n",
       " 'lit': 722,\n",
       " 'embarrassed': 723,\n",
       " 'tonight': 724,\n",
       " 'voice': 725,\n",
       " 'table': 726,\n",
       " 'park': 727,\n",
       " 'parents': 728,\n",
       " 'drop': 729,\n",
       " 'ahead': 730,\n",
       " 'agreed': 731,\n",
       " 'birds': 732,\n",
       " 'step': 733,\n",
       " 'stood': 734,\n",
       " 'ill': 735,\n",
       " 'prepared': 736,\n",
       " 'closer': 737,\n",
       " 'hated': 738,\n",
       " 'share': 739,\n",
       " 'dancing': 740,\n",
       " 'healthy': 741,\n",
       " 'talks': 742,\n",
       " 'woke': 743,\n",
       " 'cheered': 744,\n",
       " 'evil': 745,\n",
       " 'half': 746,\n",
       " 'jealous': 747,\n",
       " 'divorced': 748,\n",
       " 'angel': 749,\n",
       " 'looking': 750,\n",
       " 'control': 751,\n",
       " 'dreaming': 752,\n",
       " 'caught': 753,\n",
       " 'rope': 754,\n",
       " 'accept': 755,\n",
       " 'mom': 756,\n",
       " 'budge': 757,\n",
       " 'danger': 758,\n",
       " 'copy': 759,\n",
       " 'japan': 760,\n",
       " 'law': 761,\n",
       " 'map': 762,\n",
       " 'dirty': 763,\n",
       " 'winning': 764,\n",
       " 'form': 765,\n",
       " 'rather': 766,\n",
       " 'baby': 767,\n",
       " 'excuse': 768,\n",
       " 'continue': 769,\n",
       " 'thirty': 770,\n",
       " 'wanted': 771,\n",
       " 'surprised': 772,\n",
       " 'bank': 773,\n",
       " 'brush': 774,\n",
       " 'legs': 775,\n",
       " 'interfering': 776,\n",
       " 'ridiculous': 777,\n",
       " 'relax': 778,\n",
       " 'slow': 779,\n",
       " 'goodbye': 780,\n",
       " 'wet': 781,\n",
       " 'grab': 782,\n",
       " 'spoke': 783,\n",
       " 'runs': 784,\n",
       " 'fantastic': 785,\n",
       " 'hers': 786,\n",
       " 'tight': 787,\n",
       " 'drove': 788,\n",
       " 'screamed': 789,\n",
       " 'scared': 790,\n",
       " 'cheat': 791,\n",
       " 'glad': 792,\n",
       " 'shoot': 793,\n",
       " 'grew': 794,\n",
       " 'cars': 795,\n",
       " 'nuts': 796,\n",
       " 'age': 797,\n",
       " 'or': 798,\n",
       " 'mind': 799,\n",
       " 'innocent': 800,\n",
       " 'restless': 801,\n",
       " 'closely': 802,\n",
       " 'leg': 803,\n",
       " 'point': 804,\n",
       " 'ended': 805,\n",
       " 'dreams': 806,\n",
       " 'bird': 807,\n",
       " 'type': 808,\n",
       " 'american': 809,\n",
       " 'guilty': 810,\n",
       " 'finally': 811,\n",
       " 'soccer': 812,\n",
       " 'spring': 813,\n",
       " 'behind': 814,\n",
       " 'exhausted': 815,\n",
       " 'babbling': 816,\n",
       " 'ball': 817,\n",
       " 'knife': 818,\n",
       " 'genius': 819,\n",
       " 'glass': 820,\n",
       " 'laughing': 821,\n",
       " 'sugar': 822,\n",
       " 'correct': 823,\n",
       " 'says': 824,\n",
       " 'seemed': 825,\n",
       " 'brothers': 826,\n",
       " 'husband': 827,\n",
       " 'breathe': 828,\n",
       " 'writes': 829,\n",
       " 'matter': 830,\n",
       " 'cousin': 831,\n",
       " 'proud': 832,\n",
       " 'yesterday': 833,\n",
       " 'taught': 834,\n",
       " 'train': 835,\n",
       " 'oh': 836,\n",
       " 'bowed': 837,\n",
       " 'phoned': 838,\n",
       " 'refuse': 839,\n",
       " 'stayed': 840,\n",
       " 'lazy': 841,\n",
       " 'lies': 842,\n",
       " 'fainted': 843,\n",
       " 'then': 844,\n",
       " 'carry': 845,\n",
       " 'envy': 846,\n",
       " 'eaten': 847,\n",
       " 'sharp': 848,\n",
       " 'jumped': 849,\n",
       " 'even': 850,\n",
       " 'worry': 851,\n",
       " 'fruit': 852,\n",
       " 'rice': 853,\n",
       " 'golf': 854,\n",
       " 'unlucky': 855,\n",
       " 'escaped': 856,\n",
       " 'refused': 857,\n",
       " 'annoying': 858,\n",
       " 'doubts': 859,\n",
       " 'misled': 860,\n",
       " 'sue': 861,\n",
       " 'ears': 862,\n",
       " 'pregnant': 863,\n",
       " 'worn': 864,\n",
       " 'amazing': 865,\n",
       " 'smiling': 866,\n",
       " 'feet': 867,\n",
       " 'command': 868,\n",
       " 'built': 869,\n",
       " 'sent': 870,\n",
       " 'deny': 871,\n",
       " 'sells': 872,\n",
       " 'fear': 873,\n",
       " 'nearly': 874,\n",
       " 'rescued': 875,\n",
       " 'allow': 876,\n",
       " 'diet': 877,\n",
       " 'famous': 878,\n",
       " 'gets': 879,\n",
       " 'hoax': 880,\n",
       " 'unfair': 881,\n",
       " 'lock': 882,\n",
       " 'number': 883,\n",
       " 'stingy': 884,\n",
       " 'helping': 885,\n",
       " 'special': 886,\n",
       " 'kite': 887,\n",
       " 'sons': 888,\n",
       " 'sun': 889,\n",
       " 'teach': 890,\n",
       " 'heavy': 891,\n",
       " 'desk': 892,\n",
       " 'cooks': 893,\n",
       " 'bike': 894,\n",
       " 'ticket': 895,\n",
       " 'creep': 896,\n",
       " 'bite': 897,\n",
       " 'carefully': 898,\n",
       " 'honest': 899,\n",
       " 'death': 900,\n",
       " 'fever': 901,\n",
       " 'policeman': 902,\n",
       " 'defenseless': 903,\n",
       " 'dangerous': 904,\n",
       " 'radio': 905,\n",
       " 'sky': 906,\n",
       " 'cap': 907,\n",
       " 'singer': 908,\n",
       " 'hotel': 909,\n",
       " 'drinking': 910,\n",
       " 'intelligent': 911,\n",
       " 'pick': 912,\n",
       " 'housesitting': 913,\n",
       " 'children': 914,\n",
       " 'thing': 915,\n",
       " 'weren': 916,\n",
       " 'stuck': 917,\n",
       " 'fit': 918,\n",
       " 'brief': 919,\n",
       " 'awful': 920,\n",
       " 'burned': 921,\n",
       " 'seize': 922,\n",
       " 'voted': 923,\n",
       " 'burns': 924,\n",
       " 'human': 925,\n",
       " 'skinny': 926,\n",
       " 'happens': 927,\n",
       " 'magic': 928,\n",
       " 'windy': 929,\n",
       " 'vote': 930,\n",
       " 'deaf': 931,\n",
       " 'ugly': 932,\n",
       " 'wood': 933,\n",
       " 'decide': 934,\n",
       " 'lips': 935,\n",
       " 'loser': 936,\n",
       " 'rule': 937,\n",
       " 'thief': 938,\n",
       " 'huge': 939,\n",
       " 'crashed': 940,\n",
       " 'upset': 941,\n",
       " 'twins': 942,\n",
       " 'pity': 943,\n",
       " 'confident': 944,\n",
       " 'upstairs': 945,\n",
       " 'exist': 946,\n",
       " 'framed': 947,\n",
       " 'sushi': 948,\n",
       " 'games': 949,\n",
       " 'cab': 950,\n",
       " 'adult': 951,\n",
       " 'shopping': 952,\n",
       " 'burn': 953,\n",
       " 'shame': 954,\n",
       " 'focused': 955,\n",
       " 'boys': 956,\n",
       " 'answered': 957,\n",
       " 'canceled': 958,\n",
       " 'waste': 959,\n",
       " 'years': 960,\n",
       " 'rude': 961,\n",
       " 'tough': 962,\n",
       " 'body': 963,\n",
       " 'rush': 964,\n",
       " 'few': 965,\n",
       " 'smells': 966,\n",
       " 'cancer': 967,\n",
       " 'sports': 968,\n",
       " 'supper': 969,\n",
       " 'ride': 970,\n",
       " 'easygoing': 971,\n",
       " 'charge': 972,\n",
       " 'takes': 973,\n",
       " 'theirs': 974,\n",
       " 'adores': 975,\n",
       " 'remembers': 976,\n",
       " 'moment': 977,\n",
       " 'thinks': 978,\n",
       " 'sound': 979,\n",
       " 'cows': 980,\n",
       " 'grass': 981,\n",
       " 'towel': 982,\n",
       " 'year': 983,\n",
       " 'uncle': 984,\n",
       " 'animal': 985,\n",
       " 'disgusting': 986,\n",
       " 'borrow': 987,\n",
       " 'arrested': 988,\n",
       " 'sisters': 989,\n",
       " 'chicken': 990,\n",
       " 'travel': 991,\n",
       " 'tokyo': 992,\n",
       " 'having': 993,\n",
       " 'wide': 994,\n",
       " 'likely': 995,\n",
       " 'delicious': 996,\n",
       " 'their': 997,\n",
       " 'bill': 998,\n",
       " 'ruined': 999,\n",
       " 'missing': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang_tokenizer.word_index) + 1\n",
    "vocab_targ_size = len(target_lang_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initilize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoder name=encoder, built=False>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(64, 1024), dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_hidden = encoder.initilize_hidden_state()\n",
    "simple_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اینجا همون متد کال صدا زده میشه یعنی مدل فیت میشه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 16, 1024), dtype=float32, numpy=\n",
       " array([[[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [ 1.51093351e-03,  7.82954041e-04,  1.05450815e-03, ...,\n",
       "          -7.00181304e-03, -4.83156880e-04, -2.82977941e-03],\n",
       "         [ 4.02191235e-03,  4.49710153e-03, -5.89088770e-03, ...,\n",
       "          -5.31540718e-03,  7.27625506e-04,  1.80288672e-03],\n",
       "         ...,\n",
       "         [-2.90659582e-03,  3.13229440e-03, -8.13424028e-03, ...,\n",
       "           1.08084446e-02, -9.11219232e-03,  1.85185391e-02],\n",
       "         [-3.03838728e-03,  3.18629085e-03, -8.21223669e-03, ...,\n",
       "           1.08615328e-02, -9.08510759e-03,  1.85532328e-02],\n",
       "         [-3.11482744e-03,  3.22230067e-03, -8.25850386e-03, ...,\n",
       "           1.08887050e-02, -9.07120109e-03,  1.85656715e-02]],\n",
       " \n",
       "        [[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [-4.25209757e-04,  3.77859833e-04,  7.28090480e-03, ...,\n",
       "           1.42557407e-03,  2.15657521e-03,  4.57368838e-03],\n",
       "         [ 4.53426968e-03, -1.47045532e-03,  1.14879142e-02, ...,\n",
       "           7.35263759e-03,  9.94687621e-03,  4.76798369e-03],\n",
       "         ...,\n",
       "         [-3.02689476e-03,  3.17545165e-03, -8.11065920e-03, ...,\n",
       "           1.08492002e-02, -9.17587988e-03,  1.84417497e-02],\n",
       "         [-3.10923578e-03,  3.21229175e-03, -8.20807181e-03, ...,\n",
       "           1.08818877e-02, -9.12270322e-03,  1.85005292e-02],\n",
       "         [-3.15634860e-03,  3.23764584e-03, -8.26214626e-03, ...,\n",
       "           1.08983628e-02, -9.09315050e-03,  1.85307227e-02]],\n",
       " \n",
       "        [[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [ 5.11995330e-03,  1.11964135e-03,  7.25388015e-03, ...,\n",
       "           4.66323365e-03, -6.76458748e-03,  2.57128337e-03],\n",
       "         [ 6.40955986e-03,  9.37670469e-03,  5.78456605e-03, ...,\n",
       "           3.86370020e-03, -1.72140971e-02,  6.40633795e-03],\n",
       "         ...,\n",
       "         [-1.90963526e-03,  3.10463528e-03, -6.95401896e-03, ...,\n",
       "           1.03329625e-02, -9.95655917e-03,  1.79927722e-02],\n",
       "         [-2.42282520e-03,  3.14317830e-03, -7.50303827e-03, ...,\n",
       "           1.06382612e-02, -9.58723575e-03,  1.83586590e-02],\n",
       "         [-2.74650659e-03,  3.17254686e-03, -7.83266500e-03, ...,\n",
       "           1.07953856e-02, -9.35551897e-03,  1.85167342e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [-8.52121692e-03,  1.12675298e-02, -2.06795172e-03, ...,\n",
       "           5.96412085e-03, -1.69242267e-05,  4.29676147e-03],\n",
       "         [-5.08778729e-03, -1.58318831e-03, -7.78584881e-03, ...,\n",
       "          -6.23545051e-03,  1.02569712e-02,  1.77270477e-03],\n",
       "         ...,\n",
       "         [-3.02058551e-03,  3.19110532e-03, -8.18035658e-03, ...,\n",
       "           1.09250164e-02, -9.15822387e-03,  1.84534956e-02],\n",
       "         [-3.11026489e-03,  3.22050229e-03, -8.24602693e-03, ...,\n",
       "           1.09333079e-02, -9.11548547e-03,  1.85119770e-02],\n",
       "         [-3.16009624e-03,  3.24182585e-03, -8.28211289e-03, ...,\n",
       "           1.09317452e-02, -9.09049995e-03,  1.85401365e-02]],\n",
       " \n",
       "        [[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [ 1.29491938e-02, -5.43848285e-03,  4.04044893e-03, ...,\n",
       "          -8.85382108e-03,  2.29767524e-03, -2.03805184e-03],\n",
       "         [ 1.53874923e-02, -3.77777149e-03,  2.57838657e-03, ...,\n",
       "          -5.12803765e-03,  6.13217521e-03, -3.59638641e-03],\n",
       "         ...,\n",
       "         [-2.88786134e-03,  3.04033002e-03, -7.96077121e-03, ...,\n",
       "           1.08928885e-02, -9.20995697e-03,  1.84897184e-02],\n",
       "         [-3.03750159e-03,  3.12650274e-03, -8.11903179e-03, ...,\n",
       "           1.09209921e-02, -9.14039183e-03,  1.85489766e-02],\n",
       "         [-3.12159210e-03,  3.18286056e-03, -8.20914097e-03, ...,\n",
       "           1.09286625e-02, -9.10071842e-03,  1.85705572e-02]],\n",
       " \n",
       "        [[ 1.84819044e-03, -2.73354119e-04,  4.81493212e-03, ...,\n",
       "           4.74657770e-03, -3.15781846e-03,  1.09547726e-03],\n",
       "         [ 1.28505733e-02,  9.16590821e-03,  8.41481239e-03, ...,\n",
       "          -4.51644417e-03,  2.74900394e-03, -8.43236037e-03],\n",
       "         [-3.48996138e-03,  1.53240841e-02, -2.67812703e-03, ...,\n",
       "           1.56915211e-03,  1.16517744e-03, -7.41048250e-04],\n",
       "         ...,\n",
       "         [-2.78168125e-03,  3.09948996e-03, -7.88886286e-03, ...,\n",
       "           1.07103959e-02, -9.27678309e-03,  1.84274949e-02],\n",
       "         [-2.96088052e-03,  3.15550668e-03, -8.06625932e-03, ...,\n",
       "           1.08185839e-02, -9.17484798e-03,  1.85083374e-02],\n",
       "         [-3.06780310e-03,  3.19526577e-03, -8.17134418e-03, ...,\n",
       "           1.08721610e-02, -9.11806524e-03,  1.85441747e-02]]],\n",
       "       shape=(64, 16, 1024), dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[-0.00311483,  0.0032223 , -0.0082585 , ...,  0.01088871,\n",
       "         -0.0090712 ,  0.01856567],\n",
       "        [-0.00315635,  0.00323765, -0.00826215, ...,  0.01089836,\n",
       "         -0.00909315,  0.01853072],\n",
       "        [-0.00274651,  0.00317255, -0.00783266, ...,  0.01079539,\n",
       "         -0.00935552,  0.01851673],\n",
       "        ...,\n",
       "        [-0.0031601 ,  0.00324183, -0.00828211, ...,  0.01093175,\n",
       "         -0.0090905 ,  0.01854014],\n",
       "        [-0.00312159,  0.00318286, -0.00820914, ...,  0.01092866,\n",
       "         -0.00910072,  0.01857056],\n",
       "        [-0.0030678 ,  0.00319527, -0.00817134, ...,  0.01087216,\n",
       "         -0.00911807,  0.01854417]], shape=(64, 1024), dtype=float32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(example_input_batch, simple_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_output, simple_states = encoder(example_input_batch, simple_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.layers.Layer):\n",
    "    def __init__ (self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def call (self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        atteion_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = atteion_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, atteion_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[ 0.0015549 ,  0.00196846, -0.00438168, ...,  0.00536362,\n",
       "         -0.00445573,  0.00952649],\n",
       "        [-0.00014505,  0.00267691,  0.00034737, ...,  0.00969153,\n",
       "         -0.00435913,  0.01024607],\n",
       "        [ 0.00343958,  0.00546601, -0.00015206, ...,  0.0049861 ,\n",
       "         -0.00804402,  0.00627997],\n",
       "        ...,\n",
       "        [-0.00201446,  0.00222688, -0.00405635, ...,  0.00520826,\n",
       "         -0.00346711,  0.00872625],\n",
       "        [ 0.00201516,  0.00036857, -0.00110448, ...,  0.00629374,\n",
       "         -0.00398075,  0.00913824],\n",
       "        [-0.00023378,  0.00472771, -0.0002429 , ...,  0.00480943,\n",
       "         -0.00567034,  0.0075749 ]], shape=(64, 1024), dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 16, 1), dtype=float32, numpy=\n",
       " array([[[0.06144132],\n",
       "         [0.06141929],\n",
       "         [0.06212384],\n",
       "         ...,\n",
       "         [0.0628771 ],\n",
       "         [0.0628839 ],\n",
       "         [0.06288878]],\n",
       " \n",
       "        [[0.06157065],\n",
       "         [0.06078628],\n",
       "         [0.06225225],\n",
       "         ...,\n",
       "         [0.0629959 ],\n",
       "         [0.06300836],\n",
       "         [0.06301668]],\n",
       " \n",
       "        [[0.06103703],\n",
       "         [0.06280665],\n",
       "         [0.06245731],\n",
       "         ...,\n",
       "         [0.0624289 ],\n",
       "         [0.06244225],\n",
       "         [0.06245413]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.06124787],\n",
       "         [0.06165787],\n",
       "         [0.06166298],\n",
       "         ...,\n",
       "         [0.06267502],\n",
       "         [0.06268311],\n",
       "         [0.06268901]],\n",
       " \n",
       "        [[0.06142998],\n",
       "         [0.06129311],\n",
       "         [0.06192189],\n",
       "         ...,\n",
       "         [0.0628452 ],\n",
       "         [0.06285962],\n",
       "         [0.06286941]],\n",
       " \n",
       "        [[0.06126941],\n",
       "         [0.06039297],\n",
       "         [0.06132037],\n",
       "         ...,\n",
       "         [0.06269879],\n",
       "         [0.06270596],\n",
       "         [0.06271148]]], shape=(64, 16, 1), dtype=float32)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = Attention(10)\n",
    "attention_layer(simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_result, attention_weights = attention_layer(simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__ (self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 3727), dtype=float32, numpy=\n",
       " array([[ 1.9888279e-03, -3.9440597e-04, -1.0000544e-03, ...,\n",
       "         -6.5697427e-04, -2.9546330e-03,  2.9806781e-03],\n",
       "        [ 1.8500288e-03,  2.1690939e-04, -4.9928506e-04, ...,\n",
       "         -9.0362399e-04, -2.9271138e-03,  2.9839682e-03],\n",
       "        [ 2.5576528e-03,  4.9895630e-04, -1.2741254e-03, ...,\n",
       "         -3.1200133e-04, -2.0842957e-03,  2.6676375e-03],\n",
       "        ...,\n",
       "        [ 1.7599361e-03,  5.2244292e-04, -2.3244199e-04, ...,\n",
       "         -1.4585352e-03, -3.1376907e-03,  2.9157675e-03],\n",
       "        [ 2.3150202e-03,  5.6590070e-05, -8.0309011e-04, ...,\n",
       "         -1.2324005e-03, -2.9551946e-03,  3.2195034e-03],\n",
       "        [ 2.3689768e-03,  6.4942054e-04, -5.2455964e-04, ...,\n",
       "         -1.0549023e-03, -3.2236073e-03,  2.6269245e-03]],\n",
       "       shape=(64, 3727), dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[ 0.00093429, -0.00210999,  0.00100665, ...,  0.00597961,\n",
       "          0.00039489, -0.00394778],\n",
       "        [ 0.00087605, -0.00112863,  0.00095201, ...,  0.00628894,\n",
       "          0.00030029, -0.00431439],\n",
       "        [ 0.00016661, -0.00048174,  0.00087826, ...,  0.00516593,\n",
       "         -0.00137317, -0.00343955],\n",
       "        ...,\n",
       "        [ 0.00049306, -0.00189272,  0.0005369 , ...,  0.00593646,\n",
       "         -0.00011066, -0.00404477],\n",
       "        [ 0.00099025, -0.00157878,  0.00077572, ...,  0.00533344,\n",
       "         -0.00058259, -0.00390642],\n",
       "        [ 0.00048482, -0.00202761,  0.00159003, ...,  0.00556685,\n",
       "         -0.00077073, -0.00399681]], shape=(64, 1024), dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 16, 1), dtype=float32, numpy=\n",
       " array([[[0.06280122],\n",
       "         [0.06249231],\n",
       "         [0.0627856 ],\n",
       "         ...,\n",
       "         [0.06242182],\n",
       "         [0.06242041],\n",
       "         [0.06241943]],\n",
       " \n",
       "        [[0.06288922],\n",
       "         [0.06197404],\n",
       "         [0.06266733],\n",
       "         ...,\n",
       "         [0.06250567],\n",
       "         [0.06250572],\n",
       "         [0.06250565]],\n",
       " \n",
       "        [[0.06289238],\n",
       "         [0.06234853],\n",
       "         [0.06199121],\n",
       "         ...,\n",
       "         [0.06251831],\n",
       "         [0.06251791],\n",
       "         [0.06251565]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.06277248],\n",
       "         [0.06245122],\n",
       "         [0.06311836],\n",
       "         ...,\n",
       "         [0.06239352],\n",
       "         [0.0623923 ],\n",
       "         [0.06239137]],\n",
       " \n",
       "        [[0.06282129],\n",
       "         [0.06199877],\n",
       "         [0.06260488],\n",
       "         ...,\n",
       "         [0.06244746],\n",
       "         [0.06244493],\n",
       "         [0.0624428 ]],\n",
       " \n",
       "        [[0.06314016],\n",
       "         [0.0615673 ],\n",
       "         [0.06205253],\n",
       "         ...,\n",
       "         [0.06275924],\n",
       "         [0.06275905],\n",
       "         [0.06275816]]], shape=(64, 16, 1), dtype=float32)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(tf.random.uniform((BATCH_SIZE, 1)), simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'chckpnts'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<strat>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  4.7539096\n",
      "Epoch:  0\n",
      "Loss:  4.718105\n",
      "Epoch:  0\n",
      "Loss:  4.503395\n",
      "Epoch:  0\n",
      "Loss:  3.9899757\n",
      "Epoch:  0\n",
      "Loss:  3.371056\n",
      "Epoch:  0\n",
      "Loss:  3.1211824\n",
      "Epoch:  0\n",
      "Loss:  2.9368458\n",
      "Epoch:  0\n",
      "Loss:  2.8873913\n",
      "Epoch:  0\n",
      "Loss:  2.7312312\n",
      "Epoch:  0\n",
      "Loss:  2.7988856\n",
      "Epoch:  0\n",
      "Loss:  2.737929\n",
      "Epoch:  0\n",
      "Loss:  2.8631697\n",
      "Epoch:  0\n",
      "Loss:  2.7692966\n",
      "Epoch:  0\n",
      "Loss:  2.7470663\n",
      "Epoch:  0\n",
      "Loss:  2.7524889\n",
      "Epoch:  0\n",
      "Loss:  2.619088\n",
      "Epoch:  0\n",
      "Loss:  2.6877997\n",
      "Epoch:  0\n",
      "Loss:  2.7160268\n",
      "Epoch:  0\n",
      "Loss:  2.6671548\n",
      "Epoch:  0\n",
      "Loss:  2.6003487\n",
      "Epoch:  0\n",
      "Loss:  2.528337\n",
      "Epoch:  0\n",
      "Loss:  2.6207774\n",
      "Epoch:  0\n",
      "Loss:  2.7304873\n",
      "Epoch:  0\n",
      "Loss:  2.591435\n",
      "Epoch:  0\n",
      "Loss:  2.5190375\n",
      "Epoch:  0\n",
      "Loss:  2.3987846\n",
      "Epoch:  0\n",
      "Loss:  2.50766\n",
      "Epoch:  0\n",
      "Loss:  2.5926058\n",
      "Epoch:  0\n",
      "Loss:  2.4003062\n",
      "Epoch:  0\n",
      "Loss:  2.5822635\n",
      "Epoch:  0\n",
      "Loss:  2.5544038\n",
      "Epoch:  0\n",
      "Loss:  2.4741187\n",
      "Epoch:  0\n",
      "Loss:  2.400056\n",
      "Epoch:  0\n",
      "Loss:  2.4133217\n",
      "Epoch:  0\n",
      "Loss:  2.3439043\n",
      "Epoch:  0\n",
      "Loss:  2.3692436\n",
      "Epoch:  0\n",
      "Loss:  2.328614\n",
      "Epoch:  0\n",
      "Loss:  2.3073974\n",
      "Epoch:  0\n",
      "Loss:  2.3448887\n",
      "Epoch:  0\n",
      "Loss:  2.2628794\n",
      "Epoch:  0\n",
      "Loss:  2.4847589\n",
      "Epoch:  0\n",
      "Loss:  2.3971877\n",
      "Epoch:  0\n",
      "Loss:  2.3318238\n",
      "Epoch:  0\n",
      "Loss:  2.3493752\n",
      "Epoch:  0\n",
      "Loss:  2.3084407\n",
      "Epoch:  0\n",
      "Loss:  2.3763552\n",
      "Epoch:  0\n",
      "Loss:  2.3926604\n",
      "Epoch:  0\n",
      "Loss:  2.2632742\n",
      "Epoch:  0\n",
      "Loss:  2.3967843\n",
      "Epoch:  0\n",
      "Loss:  2.287015\n",
      "Epoch:  0\n",
      "Loss:  2.3984146\n",
      "Epoch:  0\n",
      "Loss:  2.2497697\n",
      "Epoch:  0\n",
      "Loss:  2.2540412\n",
      "Epoch:  0\n",
      "Loss:  2.3594556\n",
      "Epoch:  0\n",
      "Loss:  2.3078616\n",
      "Epoch:  0\n",
      "Loss:  2.2183754\n",
      "Epoch:  0\n",
      "Loss:  2.1803946\n",
      "Epoch:  0\n",
      "Loss:  2.325233\n",
      "Epoch:  0\n",
      "Loss:  2.2484982\n",
      "Epoch:  0\n",
      "Loss:  2.229184\n",
      "Epoch:  0\n",
      "Loss:  2.2044692\n",
      "Epoch:  0\n",
      "Loss:  2.1828754\n",
      "Epoch:  0\n",
      "Loss:  2.2667499\n",
      "Epoch:  0\n",
      "Loss:  2.2926755\n",
      "Epoch:  0\n",
      "Loss:  2.2353635\n",
      "Epoch:  0\n",
      "Loss:  2.1734347\n",
      "Epoch:  0\n",
      "Loss:  2.3539236\n",
      "Epoch:  0\n",
      "Loss:  2.1226242\n",
      "Epoch:  0\n",
      "Loss:  2.3035572\n",
      "Epoch:  0\n",
      "Loss:  2.2117047\n",
      "Epoch:  0\n",
      "Loss:  2.1528783\n",
      "Epoch:  0\n",
      "Loss:  2.2509124\n",
      "Epoch:  0\n",
      "Loss:  2.2626529\n",
      "Epoch:  0\n",
      "Loss:  2.2643301\n",
      "Epoch:  0\n",
      "Loss:  2.1666465\n",
      "Epoch:  0\n",
      "Loss:  2.215034\n",
      "Epoch:  0\n",
      "Loss:  2.1106343\n",
      "Epoch:  0\n",
      "Loss:  2.0674167\n",
      "Epoch:  0\n",
      "Loss:  2.321625\n",
      "Epoch:  0\n",
      "Loss:  2.1140676\n",
      "Epoch:  0\n",
      "Loss:  2.16529\n",
      "Epoch:  0\n",
      "Loss:  2.1688209\n",
      "Epoch:  0\n",
      "Loss:  2.2736833\n",
      "Epoch:  0\n",
      "Loss:  2.093845\n",
      "Epoch:  0\n",
      "Loss:  2.168933\n",
      "Epoch:  0\n",
      "Loss:  2.251639\n",
      "Epoch:  0\n",
      "Loss:  2.17673\n",
      "Epoch:  0\n",
      "Loss:  2.2134395\n",
      "Epoch:  0\n",
      "Loss:  2.0918813\n",
      "Epoch:  0\n",
      "Loss:  2.0750666\n",
      "Epoch:  0\n",
      "Loss:  2.1530006\n",
      "Epoch:  0\n",
      "Loss:  2.1823404\n",
      "Epoch:  0\n",
      "Loss:  2.2141685\n",
      "Epoch:  0\n",
      "Loss:  2.1209342\n",
      "Epoch:  0\n",
      "Loss:  2.1614718\n",
      "Epoch:  0\n",
      "Loss:  2.1665454\n",
      "Epoch:  0\n",
      "Loss:  2.1658103\n",
      "Epoch:  0\n",
      "Loss:  2.1131272\n",
      "Epoch:  0\n",
      "Loss:  2.1499755\n",
      "Epoch:  0\n",
      "Loss:  2.0853717\n",
      "Epoch:  0\n",
      "Loss:  2.20858\n",
      "Epoch:  0\n",
      "Loss:  2.0654397\n",
      "Epoch:  0\n",
      "Loss:  2.1310673\n",
      "Epoch:  0\n",
      "Loss:  2.0935464\n",
      "Epoch:  0\n",
      "Loss:  2.1372886\n",
      "Epoch:  0\n",
      "Loss:  2.0860229\n",
      "Epoch:  0\n",
      "Loss:  2.05465\n",
      "Epoch:  0\n",
      "Loss:  2.116618\n",
      "Epoch:  0\n",
      "Loss:  2.126913\n",
      "Epoch:  0\n",
      "Loss:  2.1106937\n",
      "Epoch:  0\n",
      "Loss:  2.0564656\n",
      "Epoch:  0\n",
      "Loss:  2.024288\n",
      "Epoch:  0\n",
      "Loss:  1.9854485\n",
      "Epoch:  0\n",
      "Loss:  1.9662845\n",
      "Epoch:  0\n",
      "Loss:  2.02483\n",
      "Epoch:  0\n",
      "Loss:  2.080727\n",
      "Epoch:  0\n",
      "Loss:  2.069214\n",
      "Epoch:  0\n",
      "Loss:  1.9873987\n",
      "Epoch:  0\n",
      "Loss:  2.0935974\n",
      "Epoch:  0\n",
      "Loss:  2.1822371\n",
      "Epoch:  0\n",
      "Loss:  2.0547276\n",
      "Epoch:  0\n",
      "Loss:  1.9429128\n",
      "Epoch:  0\n",
      "Loss:  1.9743855\n",
      "Epoch:  0\n",
      "Loss:  2.0222235\n",
      "Epoch:  0\n",
      "Loss:  1.9370884\n",
      "Epoch:  0\n",
      "Loss:  1.956666\n",
      "Epoch:  0\n",
      "Loss:  2.0287545\n",
      "Epoch:  0\n",
      "Loss:  1.9099941\n",
      "Epoch:  0\n",
      "Loss:  1.9213717\n",
      "Epoch:  0\n",
      "Loss:  1.8584764\n",
      "Epoch:  0\n",
      "Loss:  1.916123\n",
      "Epoch:  0\n",
      "Loss:  1.9329348\n",
      "Epoch:  0\n",
      "Loss:  1.9730015\n",
      "Epoch:  0\n",
      "Loss:  2.0095315\n",
      "Epoch:  0\n",
      "Loss:  1.9062611\n",
      "Epoch:  0\n",
      "Loss:  1.9129696\n",
      "Epoch:  0\n",
      "Loss:  1.946975\n",
      "Epoch:  0\n",
      "Loss:  2.0542722\n",
      "Epoch:  0\n",
      "Loss:  1.9507854\n",
      "Epoch:  0\n",
      "Loss:  1.8922621\n",
      "Epoch:  0\n",
      "Loss:  1.8251991\n",
      "Epoch:  0\n",
      "Loss:  1.8737152\n",
      "Epoch:  0\n",
      "Loss:  1.9932235\n",
      "Epoch:  0\n",
      "Loss:  1.9091066\n",
      "Epoch:  0\n",
      "Loss:  1.8488874\n",
      "Epoch:  0\n",
      "Loss:  1.8766807\n",
      "Epoch:  0\n",
      "Loss:  1.9687412\n",
      "Epoch:  0\n",
      "Loss:  1.8016884\n",
      "Epoch:  0\n",
      "Loss:  1.8535614\n",
      "Epoch:  0\n",
      "Loss:  1.8933027\n",
      "Epoch:  0\n",
      "Loss:  1.8271649\n",
      "Epoch:  0\n",
      "Loss:  1.8736217\n",
      "Epoch:  0\n",
      "Loss:  1.8392937\n",
      "Epoch:  0\n",
      "Loss:  1.7620691\n",
      "Epoch:  0\n",
      "Loss:  1.8729261\n",
      "Epoch:  0\n",
      "Loss:  1.7454077\n",
      "Epoch:  0\n",
      "Loss:  1.8867079\n",
      "Epoch:  0\n",
      "Loss:  1.8836483\n",
      "Epoch:  0\n",
      "Loss:  1.8081377\n",
      "Epoch:  0\n",
      "Loss:  1.8282831\n",
      "Epoch:  0\n",
      "Loss:  1.71612\n",
      "Epoch:  0\n",
      "Loss:  1.8878367\n",
      "Epoch:  0\n",
      "Loss:  1.804973\n",
      "Epoch:  0\n",
      "Loss:  1.8256403\n",
      "Epoch:  0\n",
      "Loss:  1.8394648\n",
      "Epoch:  0\n",
      "Loss:  1.7693285\n",
      "Epoch:  0\n",
      "Loss:  1.7445596\n",
      "Epoch:  0\n",
      "Loss:  1.8424584\n",
      "Epoch:  0\n",
      "Loss:  1.8305626\n",
      "Epoch:  0\n",
      "Loss:  1.82728\n",
      "Epoch:  0\n",
      "Loss:  1.8943403\n",
      "Epoch:  0\n",
      "Loss:  1.8290532\n",
      "Epoch:  0\n",
      "Loss:  1.7632141\n",
      "Epoch:  0\n",
      "Loss:  1.9670696\n",
      "Epoch:  0\n",
      "Loss:  1.7567093\n",
      "Epoch:  0\n",
      "Loss:  1.7312498\n",
      "Epoch:  0\n",
      "Loss:  1.7328628\n",
      "Epoch:  0\n",
      "Loss:  1.9750553\n",
      "Epoch:  0\n",
      "Loss:  1.8030164\n",
      "Epoch:  0\n",
      "Loss:  1.7357967\n",
      "Epoch:  0\n",
      "Loss:  1.8004452\n",
      "Epoch:  0\n",
      "Loss:  1.8145988\n",
      "Epoch:  0\n",
      "Loss:  1.8463764\n",
      "Epoch:  0\n",
      "Loss:  1.859865\n",
      "Epoch:  0\n",
      "Loss:  1.7430038\n",
      "Epoch:  0\n",
      "Loss:  1.6870925\n",
      "Epoch:  0\n",
      "Loss:  1.8108263\n",
      "Epoch:  0\n",
      "Loss:  1.8794264\n",
      "Epoch:  0\n",
      "Loss:  1.7854335\n",
      "Epoch:  0\n",
      "Loss:  1.6332995\n",
      "Epoch:  0\n",
      "Loss:  1.7569313\n",
      "Epoch:  0\n",
      "Loss:  1.7877789\n",
      "Epoch:  0\n",
      "Loss:  1.7121098\n",
      "Epoch:  0\n",
      "Loss:  1.7987843\n",
      "Epoch:  0\n",
      "Loss:  1.8301907\n",
      "Epoch:  0\n",
      "Loss:  1.8013681\n",
      "Epoch:  0\n",
      "Loss:  1.8091825\n",
      "Epoch:  0\n",
      "Loss:  1.6771421\n",
      "Epoch:  0\n",
      "Loss:  1.6964483\n",
      "Epoch:  0\n",
      "Loss:  1.663285\n",
      "Epoch:  0\n",
      "Loss:  1.9920069\n",
      "Epoch:  0\n",
      "Loss:  1.8307368\n",
      "Epoch:  0\n",
      "Loss:  1.6875395\n",
      "Epoch:  0\n",
      "Loss:  1.7345505\n",
      "Epoch:  0\n",
      "Loss:  1.833183\n",
      "Epoch:  0\n",
      "Loss:  1.788023\n",
      "Epoch:  0\n",
      "Loss:  1.7408568\n",
      "Epoch:  0\n",
      "Loss:  1.77714\n",
      "Epoch:  0\n",
      "Loss:  1.5931945\n",
      "Epoch:  0\n",
      "Loss:  1.6508293\n",
      "Epoch:  0\n",
      "Loss:  1.5705339\n",
      "Epoch:  0\n",
      "Loss:  1.8209722\n",
      "Epoch:  0\n",
      "Loss:  1.7467556\n",
      "Epoch:  0\n",
      "Loss:  1.726804\n",
      "Epoch:  0\n",
      "Loss:  1.8060776\n",
      "Epoch:  0\n",
      "Loss:  1.7166704\n",
      "Epoch:  0\n",
      "Loss:  1.8411099\n",
      "Epoch:  0\n",
      "Loss:  1.7574995\n",
      "Epoch:  0\n",
      "Loss:  1.6940153\n",
      "Epoch:  0\n",
      "Loss:  1.7685826\n",
      "Epoch:  0\n",
      "Loss:  1.7542499\n",
      "Epoch:  0\n",
      "Loss:  1.8265053\n",
      "Epoch:  0\n",
      "Loss:  1.8757375\n",
      "Epoch:  0\n",
      "Loss:  1.8028218\n",
      "Epoch:  0\n",
      "Loss:  1.765716\n",
      "Epoch:  0\n",
      "Loss:  1.7978528\n",
      "Epoch:  0\n",
      "Loss:  1.7586397\n",
      "Epoch:  0\n",
      "Loss:  1.643897\n",
      "Epoch:  0\n",
      "Loss:  1.6570383\n",
      "Epoch:  0\n",
      "Loss:  1.6227224\n",
      "Epoch:  0\n",
      "Loss:  1.7250097\n",
      "Epoch:  0\n",
      "Loss:  1.7969892\n",
      "Epoch:  0\n",
      "Loss:  1.6374394\n",
      "Epoch:  0\n",
      "Loss:  1.7563725\n",
      "Epoch:  0\n",
      "Loss:  1.7343876\n",
      "Epoch:  0\n",
      "Loss:  1.6744524\n",
      "Epoch:  0\n",
      "Loss:  1.8365784\n",
      "Epoch:  0\n",
      "Loss:  1.583637\n",
      "Epoch:  0\n",
      "Loss:  1.6722286\n",
      "Epoch:  0\n",
      "Loss:  1.7416952\n",
      "Epoch:  0\n",
      "Loss:  1.7673604\n",
      "Epoch:  0\n",
      "Loss:  1.6430471\n",
      "Epoch:  0\n",
      "Loss:  1.86225\n",
      "Epoch:  0\n",
      "Loss:  1.6584003\n",
      "Epoch:  0\n",
      "Loss:  1.7156938\n",
      "Epoch:  0\n",
      "Loss:  1.7364848\n",
      "Epoch:  0\n",
      "Loss:  1.6831417\n",
      "Epoch:  0\n",
      "Loss:  1.7151892\n",
      "Epoch:  0\n",
      "Loss:  1.7104698\n",
      "Epoch:  0\n",
      "Loss:  1.8005543\n",
      "Epoch:  1\n",
      "Loss:  1.6975701\n",
      "Epoch:  1\n",
      "Loss:  1.7093064\n",
      "Epoch:  1\n",
      "Loss:  1.6495384\n",
      "Epoch:  1\n",
      "Loss:  1.5796225\n",
      "Epoch:  1\n",
      "Loss:  1.6640899\n",
      "Epoch:  1\n",
      "Loss:  1.6580098\n",
      "Epoch:  1\n",
      "Loss:  1.522213\n",
      "Epoch:  1\n",
      "Loss:  1.6430504\n",
      "Epoch:  1\n",
      "Loss:  1.622431\n",
      "Epoch:  1\n",
      "Loss:  1.623361\n",
      "Epoch:  1\n",
      "Loss:  1.6678501\n",
      "Epoch:  1\n",
      "Loss:  1.669075\n",
      "Epoch:  1\n",
      "Loss:  1.5990993\n",
      "Epoch:  1\n",
      "Loss:  1.5723088\n",
      "Epoch:  1\n",
      "Loss:  1.6391666\n",
      "Epoch:  1\n",
      "Loss:  1.6195209\n",
      "Epoch:  1\n",
      "Loss:  1.4485016\n",
      "Epoch:  1\n",
      "Loss:  1.6086572\n",
      "Epoch:  1\n",
      "Loss:  1.6174717\n",
      "Epoch:  1\n",
      "Loss:  1.6752799\n",
      "Epoch:  1\n",
      "Loss:  1.6042621\n",
      "Epoch:  1\n",
      "Loss:  1.6121473\n",
      "Epoch:  1\n",
      "Loss:  1.6119776\n",
      "Epoch:  1\n",
      "Loss:  1.630624\n",
      "Epoch:  1\n",
      "Loss:  1.6159012\n",
      "Epoch:  1\n",
      "Loss:  1.5667512\n",
      "Epoch:  1\n",
      "Loss:  1.6261276\n",
      "Epoch:  1\n",
      "Loss:  1.5960948\n",
      "Epoch:  1\n",
      "Loss:  1.623555\n",
      "Epoch:  1\n",
      "Loss:  1.5965645\n",
      "Epoch:  1\n",
      "Loss:  1.587897\n",
      "Epoch:  1\n",
      "Loss:  1.6435928\n",
      "Epoch:  1\n",
      "Loss:  1.566904\n",
      "Epoch:  1\n",
      "Loss:  1.575967\n",
      "Epoch:  1\n",
      "Loss:  1.5124829\n",
      "Epoch:  1\n",
      "Loss:  1.6750975\n",
      "Epoch:  1\n",
      "Loss:  1.5059093\n",
      "Epoch:  1\n",
      "Loss:  1.6047989\n",
      "Epoch:  1\n",
      "Loss:  1.652002\n",
      "Epoch:  1\n",
      "Loss:  1.5626552\n",
      "Epoch:  1\n",
      "Loss:  1.550189\n",
      "Epoch:  1\n",
      "Loss:  1.6577377\n",
      "Epoch:  1\n",
      "Loss:  1.5323784\n",
      "Epoch:  1\n",
      "Loss:  1.5903612\n",
      "Epoch:  1\n",
      "Loss:  1.6739833\n",
      "Epoch:  1\n",
      "Loss:  1.5677922\n",
      "Epoch:  1\n",
      "Loss:  1.5844009\n",
      "Epoch:  1\n",
      "Loss:  1.5874041\n",
      "Epoch:  1\n",
      "Loss:  1.5601404\n",
      "Epoch:  1\n",
      "Loss:  1.6688309\n",
      "Epoch:  1\n",
      "Loss:  1.5872453\n",
      "Epoch:  1\n",
      "Loss:  1.6417351\n",
      "Epoch:  1\n",
      "Loss:  1.5584123\n",
      "Epoch:  1\n",
      "Loss:  1.5392371\n",
      "Epoch:  1\n",
      "Loss:  1.582591\n",
      "Epoch:  1\n",
      "Loss:  1.6349518\n",
      "Epoch:  1\n",
      "Loss:  1.6434009\n",
      "Epoch:  1\n",
      "Loss:  1.5381844\n",
      "Epoch:  1\n",
      "Loss:  1.5947604\n",
      "Epoch:  1\n",
      "Loss:  1.6567008\n",
      "Epoch:  1\n",
      "Loss:  1.5516045\n",
      "Epoch:  1\n",
      "Loss:  1.5440241\n",
      "Epoch:  1\n",
      "Loss:  1.4600201\n",
      "Epoch:  1\n",
      "Loss:  1.5038036\n",
      "Epoch:  1\n",
      "Loss:  1.5346558\n",
      "Epoch:  1\n",
      "Loss:  1.5938542\n",
      "Epoch:  1\n",
      "Loss:  1.4875953\n",
      "Epoch:  1\n",
      "Loss:  1.6458035\n",
      "Epoch:  1\n",
      "Loss:  1.6116025\n",
      "Epoch:  1\n",
      "Loss:  1.409438\n",
      "Epoch:  1\n",
      "Loss:  1.6888431\n",
      "Epoch:  1\n",
      "Loss:  1.5251046\n",
      "Epoch:  1\n",
      "Loss:  1.4928803\n",
      "Epoch:  1\n",
      "Loss:  1.5331258\n",
      "Epoch:  1\n",
      "Loss:  1.5699568\n",
      "Epoch:  1\n",
      "Loss:  1.5411313\n",
      "Epoch:  1\n",
      "Loss:  1.4925218\n",
      "Epoch:  1\n",
      "Loss:  1.6431271\n",
      "Epoch:  1\n",
      "Loss:  1.5637487\n",
      "Epoch:  1\n",
      "Loss:  1.5524361\n",
      "Epoch:  1\n",
      "Loss:  1.5162401\n",
      "Epoch:  1\n",
      "Loss:  1.5891186\n",
      "Epoch:  1\n",
      "Loss:  1.5196751\n",
      "Epoch:  1\n",
      "Loss:  1.5388883\n",
      "Epoch:  1\n",
      "Loss:  1.5290372\n",
      "Epoch:  1\n",
      "Loss:  1.5273054\n",
      "Epoch:  1\n",
      "Loss:  1.5432613\n",
      "Epoch:  1\n",
      "Loss:  1.5154507\n",
      "Epoch:  1\n",
      "Loss:  1.4690634\n",
      "Epoch:  1\n",
      "Loss:  1.5789396\n",
      "Epoch:  1\n",
      "Loss:  1.5186943\n",
      "Epoch:  1\n",
      "Loss:  1.5303941\n",
      "Epoch:  1\n",
      "Loss:  1.4942685\n",
      "Epoch:  1\n",
      "Loss:  1.4571406\n",
      "Epoch:  1\n",
      "Loss:  1.5163491\n",
      "Epoch:  1\n",
      "Loss:  1.5694509\n",
      "Epoch:  1\n",
      "Loss:  1.4871414\n",
      "Epoch:  1\n",
      "Loss:  1.5668676\n",
      "Epoch:  1\n",
      "Loss:  1.5522693\n",
      "Epoch:  1\n",
      "Loss:  1.4576046\n",
      "Epoch:  1\n",
      "Loss:  1.5133901\n",
      "Epoch:  1\n",
      "Loss:  1.5356722\n",
      "Epoch:  1\n",
      "Loss:  1.537724\n",
      "Epoch:  1\n",
      "Loss:  1.4720534\n",
      "Epoch:  1\n",
      "Loss:  1.462931\n",
      "Epoch:  1\n",
      "Loss:  1.4848024\n",
      "Epoch:  1\n",
      "Loss:  1.4100792\n",
      "Epoch:  1\n",
      "Loss:  1.4312468\n",
      "Epoch:  1\n",
      "Loss:  1.3735691\n",
      "Epoch:  1\n",
      "Loss:  1.6578333\n",
      "Epoch:  1\n",
      "Loss:  1.54797\n",
      "Epoch:  1\n",
      "Loss:  1.5543545\n",
      "Epoch:  1\n",
      "Loss:  1.5214351\n",
      "Epoch:  1\n",
      "Loss:  1.4617485\n",
      "Epoch:  1\n",
      "Loss:  1.5147145\n",
      "Epoch:  1\n",
      "Loss:  1.5711715\n",
      "Epoch:  1\n",
      "Loss:  1.4999173\n",
      "Epoch:  1\n",
      "Loss:  1.4646289\n",
      "Epoch:  1\n",
      "Loss:  1.5061387\n",
      "Epoch:  1\n",
      "Loss:  1.5558894\n",
      "Epoch:  1\n",
      "Loss:  1.5334547\n",
      "Epoch:  1\n",
      "Loss:  1.4867132\n",
      "Epoch:  1\n",
      "Loss:  1.4364603\n",
      "Epoch:  1\n",
      "Loss:  1.4848516\n",
      "Epoch:  1\n",
      "Loss:  1.52444\n",
      "Epoch:  1\n",
      "Loss:  1.5500308\n",
      "Epoch:  1\n",
      "Loss:  1.5493515\n",
      "Epoch:  1\n",
      "Loss:  1.4785883\n",
      "Epoch:  1\n",
      "Loss:  1.4642656\n",
      "Epoch:  1\n",
      "Loss:  1.4987231\n",
      "Epoch:  1\n",
      "Loss:  1.3938787\n",
      "Epoch:  1\n",
      "Loss:  1.4394476\n",
      "Epoch:  1\n",
      "Loss:  1.5342548\n",
      "Epoch:  1\n",
      "Loss:  1.5268396\n",
      "Epoch:  1\n",
      "Loss:  1.5761938\n",
      "Epoch:  1\n",
      "Loss:  1.4205239\n",
      "Epoch:  1\n",
      "Loss:  1.4037223\n",
      "Epoch:  1\n",
      "Loss:  1.4708091\n",
      "Epoch:  1\n",
      "Loss:  1.3279536\n",
      "Epoch:  1\n",
      "Loss:  1.3864748\n",
      "Epoch:  1\n",
      "Loss:  1.4479105\n",
      "Epoch:  1\n",
      "Loss:  1.5221219\n",
      "Epoch:  1\n",
      "Loss:  1.5478793\n",
      "Epoch:  1\n",
      "Loss:  1.3648312\n",
      "Epoch:  1\n",
      "Loss:  1.4720159\n",
      "Epoch:  1\n",
      "Loss:  1.4680828\n",
      "Epoch:  1\n",
      "Loss:  1.4124407\n",
      "Epoch:  1\n",
      "Loss:  1.6154616\n",
      "Epoch:  1\n",
      "Loss:  1.5608653\n",
      "Epoch:  1\n",
      "Loss:  1.3859935\n",
      "Epoch:  1\n",
      "Loss:  1.4631398\n",
      "Epoch:  1\n",
      "Loss:  1.5513694\n",
      "Epoch:  1\n",
      "Loss:  1.5229131\n",
      "Epoch:  1\n",
      "Loss:  1.4273169\n",
      "Epoch:  1\n",
      "Loss:  1.5362709\n",
      "Epoch:  1\n",
      "Loss:  1.4897128\n",
      "Epoch:  1\n",
      "Loss:  1.36794\n",
      "Epoch:  1\n",
      "Loss:  1.4607793\n",
      "Epoch:  1\n",
      "Loss:  1.4345671\n",
      "Epoch:  1\n",
      "Loss:  1.4870554\n",
      "Epoch:  1\n",
      "Loss:  1.501526\n",
      "Epoch:  1\n",
      "Loss:  1.4838431\n",
      "Epoch:  1\n",
      "Loss:  1.3677036\n",
      "Epoch:  1\n",
      "Loss:  1.5328996\n",
      "Epoch:  1\n",
      "Loss:  1.5276301\n",
      "Epoch:  1\n",
      "Loss:  1.4667085\n",
      "Epoch:  1\n",
      "Loss:  1.4723566\n",
      "Epoch:  1\n",
      "Loss:  1.6117818\n",
      "Epoch:  1\n",
      "Loss:  1.4344952\n",
      "Epoch:  1\n",
      "Loss:  1.3668702\n",
      "Epoch:  1\n",
      "Loss:  1.3755796\n",
      "Epoch:  1\n",
      "Loss:  1.3656573\n",
      "Epoch:  1\n",
      "Loss:  1.559249\n",
      "Epoch:  1\n",
      "Loss:  1.4826784\n",
      "Epoch:  1\n",
      "Loss:  1.4097843\n",
      "Epoch:  1\n",
      "Loss:  1.5090904\n",
      "Epoch:  1\n",
      "Loss:  1.488833\n",
      "Epoch:  1\n",
      "Loss:  1.472841\n",
      "Epoch:  1\n",
      "Loss:  1.5225663\n",
      "Epoch:  1\n",
      "Loss:  1.3644173\n",
      "Epoch:  1\n",
      "Loss:  1.4433727\n",
      "Epoch:  1\n",
      "Loss:  1.3839061\n",
      "Epoch:  1\n",
      "Loss:  1.3581616\n",
      "Epoch:  1\n",
      "Loss:  1.41744\n",
      "Epoch:  1\n",
      "Loss:  1.4436761\n",
      "Epoch:  1\n",
      "Loss:  1.4860929\n",
      "Epoch:  1\n",
      "Loss:  1.4320388\n",
      "Epoch:  1\n",
      "Loss:  1.3980572\n",
      "Epoch:  1\n",
      "Loss:  1.2837064\n",
      "Epoch:  1\n",
      "Loss:  1.3339136\n",
      "Epoch:  1\n",
      "Loss:  1.396526\n",
      "Epoch:  1\n",
      "Loss:  1.2707659\n",
      "Epoch:  1\n",
      "Loss:  1.4390075\n",
      "Epoch:  1\n",
      "Loss:  1.3517563\n",
      "Epoch:  1\n",
      "Loss:  1.292344\n",
      "Epoch:  1\n",
      "Loss:  1.4183141\n",
      "Epoch:  1\n",
      "Loss:  1.4701172\n",
      "Epoch:  1\n",
      "Loss:  1.440478\n",
      "Epoch:  1\n",
      "Loss:  1.4828619\n",
      "Epoch:  1\n",
      "Loss:  1.3920491\n",
      "Epoch:  1\n",
      "Loss:  1.5278141\n",
      "Epoch:  1\n",
      "Loss:  1.453389\n",
      "Epoch:  1\n",
      "Loss:  1.3349928\n",
      "Epoch:  1\n",
      "Loss:  1.4257699\n",
      "Epoch:  1\n",
      "Loss:  1.3671829\n",
      "Epoch:  1\n",
      "Loss:  1.2830449\n",
      "Epoch:  1\n",
      "Loss:  1.467363\n",
      "Epoch:  1\n",
      "Loss:  1.3787423\n",
      "Epoch:  1\n",
      "Loss:  1.3775556\n",
      "Epoch:  1\n",
      "Loss:  1.4108982\n",
      "Epoch:  1\n",
      "Loss:  1.3278447\n",
      "Epoch:  1\n",
      "Loss:  1.4175932\n",
      "Epoch:  1\n",
      "Loss:  1.399239\n",
      "Epoch:  1\n",
      "Loss:  1.4271926\n",
      "Epoch:  1\n",
      "Loss:  1.4468782\n",
      "Epoch:  1\n",
      "Loss:  1.4634621\n",
      "Epoch:  1\n",
      "Loss:  1.3059514\n",
      "Epoch:  1\n",
      "Loss:  1.2868063\n",
      "Epoch:  1\n",
      "Loss:  1.3195379\n",
      "Epoch:  1\n",
      "Loss:  1.4134539\n",
      "Epoch:  1\n",
      "Loss:  1.2505599\n",
      "Epoch:  1\n",
      "Loss:  1.3682759\n",
      "Epoch:  1\n",
      "Loss:  1.4355781\n",
      "Epoch:  1\n",
      "Loss:  1.4646366\n",
      "Epoch:  1\n",
      "Loss:  1.4399371\n",
      "Epoch:  1\n",
      "Loss:  1.3378122\n",
      "Epoch:  1\n",
      "Loss:  1.4535291\n",
      "Epoch:  1\n",
      "Loss:  1.321669\n",
      "Epoch:  1\n",
      "Loss:  1.4087001\n",
      "Epoch:  1\n",
      "Loss:  1.4487853\n",
      "Epoch:  1\n",
      "Loss:  1.4959519\n",
      "Epoch:  1\n",
      "Loss:  1.4469895\n",
      "Epoch:  1\n",
      "Loss:  1.394527\n",
      "Epoch:  1\n",
      "Loss:  1.2868904\n",
      "Epoch:  1\n",
      "Loss:  1.3767011\n",
      "Epoch:  1\n",
      "Loss:  1.3776367\n",
      "Epoch:  1\n",
      "Loss:  1.3391658\n",
      "Epoch:  1\n",
      "Loss:  1.4658573\n",
      "Epoch:  1\n",
      "Loss:  1.3130491\n",
      "Epoch:  1\n",
      "Loss:  1.3342346\n",
      "Epoch:  1\n",
      "Loss:  1.3538401\n",
      "Epoch:  1\n",
      "Loss:  1.3892882\n",
      "Epoch:  1\n",
      "Loss:  1.3304485\n",
      "Epoch:  1\n",
      "Loss:  1.3762562\n",
      "Epoch:  1\n",
      "Loss:  1.288008\n",
      "Epoch:  1\n",
      "Loss:  1.42274\n",
      "Epoch:  1\n",
      "Loss:  1.2457912\n",
      "Epoch:  1\n",
      "Loss:  1.2480323\n",
      "Epoch:  1\n",
      "Loss:  1.3833603\n",
      "Epoch:  1\n",
      "Loss:  1.4268897\n",
      "Epoch:  2\n",
      "Loss:  1.2627486\n",
      "Epoch:  2\n",
      "Loss:  1.1147052\n",
      "Epoch:  2\n",
      "Loss:  1.2087991\n",
      "Epoch:  2\n",
      "Loss:  1.2440733\n",
      "Epoch:  2\n",
      "Loss:  1.3238268\n",
      "Epoch:  2\n",
      "Loss:  1.1363249\n",
      "Epoch:  2\n",
      "Loss:  1.3003969\n",
      "Epoch:  2\n",
      "Loss:  1.2168648\n",
      "Epoch:  2\n",
      "Loss:  1.2949249\n",
      "Epoch:  2\n",
      "Loss:  1.22785\n",
      "Epoch:  2\n",
      "Loss:  1.3352975\n",
      "Epoch:  2\n",
      "Loss:  1.2543924\n",
      "Epoch:  2\n",
      "Loss:  1.3587296\n",
      "Epoch:  2\n",
      "Loss:  1.1889242\n",
      "Epoch:  2\n",
      "Loss:  1.2086624\n",
      "Epoch:  2\n",
      "Loss:  1.3434619\n",
      "Epoch:  2\n",
      "Loss:  1.2223778\n",
      "Epoch:  2\n",
      "Loss:  1.1142249\n",
      "Epoch:  2\n",
      "Loss:  1.2659483\n",
      "Epoch:  2\n",
      "Loss:  1.2253629\n",
      "Epoch:  2\n",
      "Loss:  1.3801178\n",
      "Epoch:  2\n",
      "Loss:  1.2053844\n",
      "Epoch:  2\n",
      "Loss:  1.3032335\n",
      "Epoch:  2\n",
      "Loss:  1.2708184\n",
      "Epoch:  2\n",
      "Loss:  1.2326252\n",
      "Epoch:  2\n",
      "Loss:  1.2485553\n",
      "Epoch:  2\n",
      "Loss:  1.3006232\n",
      "Epoch:  2\n",
      "Loss:  1.2358085\n",
      "Epoch:  2\n",
      "Loss:  1.1906976\n",
      "Epoch:  2\n",
      "Loss:  1.1380858\n",
      "Epoch:  2\n",
      "Loss:  1.2871203\n",
      "Epoch:  2\n",
      "Loss:  1.2387416\n",
      "Epoch:  2\n",
      "Loss:  1.2063817\n",
      "Epoch:  2\n",
      "Loss:  1.2177287\n",
      "Epoch:  2\n",
      "Loss:  1.1848218\n",
      "Epoch:  2\n",
      "Loss:  1.257733\n",
      "Epoch:  2\n",
      "Loss:  1.2722797\n",
      "Epoch:  2\n",
      "Loss:  1.2380416\n",
      "Epoch:  2\n",
      "Loss:  1.2677257\n",
      "Epoch:  2\n",
      "Loss:  1.1454542\n",
      "Epoch:  2\n",
      "Loss:  1.2567286\n",
      "Epoch:  2\n",
      "Loss:  1.2167962\n",
      "Epoch:  2\n",
      "Loss:  1.2455467\n",
      "Epoch:  2\n",
      "Loss:  1.1607976\n",
      "Epoch:  2\n",
      "Loss:  1.1254333\n",
      "Epoch:  2\n",
      "Loss:  1.1341227\n",
      "Epoch:  2\n",
      "Loss:  1.2592604\n",
      "Epoch:  2\n",
      "Loss:  1.2243034\n",
      "Epoch:  2\n",
      "Loss:  1.248745\n",
      "Epoch:  2\n",
      "Loss:  1.2292002\n",
      "Epoch:  2\n",
      "Loss:  1.1964788\n",
      "Epoch:  2\n",
      "Loss:  1.1434329\n",
      "Epoch:  2\n",
      "Loss:  1.3123308\n",
      "Epoch:  2\n",
      "Loss:  1.0965538\n",
      "Epoch:  2\n",
      "Loss:  1.2100897\n",
      "Epoch:  2\n",
      "Loss:  1.1674078\n",
      "Epoch:  2\n",
      "Loss:  1.2216599\n",
      "Epoch:  2\n",
      "Loss:  1.2959931\n",
      "Epoch:  2\n",
      "Loss:  1.2591836\n",
      "Epoch:  2\n",
      "Loss:  1.1682543\n",
      "Epoch:  2\n",
      "Loss:  1.2022299\n",
      "Epoch:  2\n",
      "Loss:  1.233858\n",
      "Epoch:  2\n",
      "Loss:  1.1209068\n",
      "Epoch:  2\n",
      "Loss:  1.1903784\n",
      "Epoch:  2\n",
      "Loss:  1.0511647\n",
      "Epoch:  2\n",
      "Loss:  1.2293798\n",
      "Epoch:  2\n",
      "Loss:  1.148745\n",
      "Epoch:  2\n",
      "Loss:  1.2735468\n",
      "Epoch:  2\n",
      "Loss:  1.2853491\n",
      "Epoch:  2\n",
      "Loss:  1.1653267\n",
      "Epoch:  2\n",
      "Loss:  1.1370223\n",
      "Epoch:  2\n",
      "Loss:  1.1481647\n",
      "Epoch:  2\n",
      "Loss:  1.2800219\n",
      "Epoch:  2\n",
      "Loss:  1.1427276\n",
      "Epoch:  2\n",
      "Loss:  1.1645396\n",
      "Epoch:  2\n",
      "Loss:  1.2491286\n",
      "Epoch:  2\n",
      "Loss:  1.2145574\n",
      "Epoch:  2\n",
      "Loss:  1.2601898\n",
      "Epoch:  2\n",
      "Loss:  1.1940826\n",
      "Epoch:  2\n",
      "Loss:  1.2923265\n",
      "Epoch:  2\n",
      "Loss:  1.1559993\n",
      "Epoch:  2\n",
      "Loss:  1.1271867\n",
      "Epoch:  2\n",
      "Loss:  1.220416\n",
      "Epoch:  2\n",
      "Loss:  1.1814985\n",
      "Epoch:  2\n",
      "Loss:  1.234487\n",
      "Epoch:  2\n",
      "Loss:  1.2685815\n",
      "Epoch:  2\n",
      "Loss:  1.1413686\n",
      "Epoch:  2\n",
      "Loss:  1.2397325\n",
      "Epoch:  2\n",
      "Loss:  1.1974854\n",
      "Epoch:  2\n",
      "Loss:  1.2183927\n",
      "Epoch:  2\n",
      "Loss:  1.1578556\n",
      "Epoch:  2\n",
      "Loss:  1.1290603\n",
      "Epoch:  2\n",
      "Loss:  1.2060032\n",
      "Epoch:  2\n",
      "Loss:  1.2596389\n",
      "Epoch:  2\n",
      "Loss:  1.2340994\n",
      "Epoch:  2\n",
      "Loss:  1.2286123\n",
      "Epoch:  2\n",
      "Loss:  1.2083079\n",
      "Epoch:  2\n",
      "Loss:  1.1435345\n",
      "Epoch:  2\n",
      "Loss:  1.150933\n",
      "Epoch:  2\n",
      "Loss:  1.1296555\n",
      "Epoch:  2\n",
      "Loss:  1.1585535\n",
      "Epoch:  2\n",
      "Loss:  1.1770275\n",
      "Epoch:  2\n",
      "Loss:  1.1849773\n",
      "Epoch:  2\n",
      "Loss:  1.1976025\n",
      "Epoch:  2\n",
      "Loss:  1.1276485\n",
      "Epoch:  2\n",
      "Loss:  1.1992897\n",
      "Epoch:  2\n",
      "Loss:  1.0008684\n",
      "Epoch:  2\n",
      "Loss:  1.141198\n",
      "Epoch:  2\n",
      "Loss:  1.0547973\n",
      "Epoch:  2\n",
      "Loss:  1.1972463\n",
      "Epoch:  2\n",
      "Loss:  1.3063034\n",
      "Epoch:  2\n",
      "Loss:  1.2354647\n",
      "Epoch:  2\n",
      "Loss:  1.196555\n",
      "Epoch:  2\n",
      "Loss:  1.1411977\n",
      "Epoch:  2\n",
      "Loss:  1.1629786\n",
      "Epoch:  2\n",
      "Loss:  1.2065318\n",
      "Epoch:  2\n",
      "Loss:  1.1981201\n",
      "Epoch:  2\n",
      "Loss:  1.1014521\n",
      "Epoch:  2\n",
      "Loss:  1.1980479\n",
      "Epoch:  2\n",
      "Loss:  1.2895104\n",
      "Epoch:  2\n",
      "Loss:  1.2297876\n",
      "Epoch:  2\n",
      "Loss:  1.1790491\n",
      "Epoch:  2\n",
      "Loss:  1.2286354\n",
      "Epoch:  2\n",
      "Loss:  1.2394162\n",
      "Epoch:  2\n",
      "Loss:  1.1384499\n",
      "Epoch:  2\n",
      "Loss:  1.2014505\n",
      "Epoch:  2\n",
      "Loss:  1.0610082\n",
      "Epoch:  2\n",
      "Loss:  1.2055254\n",
      "Epoch:  2\n",
      "Loss:  1.1044495\n",
      "Epoch:  2\n",
      "Loss:  1.2723634\n",
      "Epoch:  2\n",
      "Loss:  1.2332022\n",
      "Epoch:  2\n",
      "Loss:  1.2670393\n",
      "Epoch:  2\n",
      "Loss:  1.2880808\n",
      "Epoch:  2\n",
      "Loss:  1.2250941\n",
      "Epoch:  2\n",
      "Loss:  1.1948987\n",
      "Epoch:  2\n",
      "Loss:  1.239186\n",
      "Epoch:  2\n",
      "Loss:  1.1979296\n",
      "Epoch:  2\n",
      "Loss:  1.2065731\n",
      "Epoch:  2\n",
      "Loss:  1.2684023\n",
      "Epoch:  2\n",
      "Loss:  1.1233547\n",
      "Epoch:  2\n",
      "Loss:  1.1236736\n",
      "Epoch:  2\n",
      "Loss:  1.158839\n",
      "Epoch:  2\n",
      "Loss:  1.1608628\n",
      "Epoch:  2\n",
      "Loss:  1.121609\n",
      "Epoch:  2\n",
      "Loss:  1.1446642\n",
      "Epoch:  2\n",
      "Loss:  1.2581002\n",
      "Epoch:  2\n",
      "Loss:  1.2217991\n",
      "Epoch:  2\n",
      "Loss:  1.1117725\n",
      "Epoch:  2\n",
      "Loss:  1.0840005\n",
      "Epoch:  2\n",
      "Loss:  1.1811973\n",
      "Epoch:  2\n",
      "Loss:  1.0550302\n",
      "Epoch:  2\n",
      "Loss:  1.1369946\n",
      "Epoch:  2\n",
      "Loss:  1.1168233\n",
      "Epoch:  2\n",
      "Loss:  1.1454706\n",
      "Epoch:  2\n",
      "Loss:  1.1807623\n",
      "Epoch:  2\n",
      "Loss:  1.0818698\n",
      "Epoch:  2\n",
      "Loss:  1.0272473\n",
      "Epoch:  2\n",
      "Loss:  1.167322\n",
      "Epoch:  2\n",
      "Loss:  1.2363205\n",
      "Epoch:  2\n",
      "Loss:  1.1382787\n",
      "Epoch:  2\n",
      "Loss:  1.1078174\n",
      "Epoch:  2\n",
      "Loss:  1.1379964\n",
      "Epoch:  2\n",
      "Loss:  1.0858066\n",
      "Epoch:  2\n",
      "Loss:  1.095278\n",
      "Epoch:  2\n",
      "Loss:  1.122749\n",
      "Epoch:  2\n",
      "Loss:  1.0953529\n",
      "Epoch:  2\n",
      "Loss:  1.1164356\n",
      "Epoch:  2\n",
      "Loss:  1.2038531\n",
      "Epoch:  2\n",
      "Loss:  1.1206056\n",
      "Epoch:  2\n",
      "Loss:  1.1528044\n",
      "Epoch:  2\n",
      "Loss:  1.2272413\n",
      "Epoch:  2\n",
      "Loss:  1.1483166\n",
      "Epoch:  2\n",
      "Loss:  1.2186183\n",
      "Epoch:  2\n",
      "Loss:  1.117715\n",
      "Epoch:  2\n",
      "Loss:  1.1921611\n",
      "Epoch:  2\n",
      "Loss:  1.1650574\n",
      "Epoch:  2\n",
      "Loss:  1.1141877\n",
      "Epoch:  2\n",
      "Loss:  1.2103788\n",
      "Epoch:  2\n",
      "Loss:  1.127345\n",
      "Epoch:  2\n",
      "Loss:  1.1160182\n",
      "Epoch:  2\n",
      "Loss:  1.1542419\n",
      "Epoch:  2\n",
      "Loss:  1.1362609\n",
      "Epoch:  2\n",
      "Loss:  1.1147726\n",
      "Epoch:  2\n",
      "Loss:  1.0888141\n",
      "Epoch:  2\n",
      "Loss:  1.1389999\n",
      "Epoch:  2\n",
      "Loss:  1.1088728\n",
      "Epoch:  2\n",
      "Loss:  1.179734\n",
      "Epoch:  2\n",
      "Loss:  1.1675327\n",
      "Epoch:  2\n",
      "Loss:  1.054727\n",
      "Epoch:  2\n",
      "Loss:  1.1231124\n",
      "Epoch:  2\n",
      "Loss:  1.0956212\n",
      "Epoch:  2\n",
      "Loss:  1.1411204\n",
      "Epoch:  2\n",
      "Loss:  1.1187739\n",
      "Epoch:  2\n",
      "Loss:  1.1370833\n",
      "Epoch:  2\n",
      "Loss:  1.1141509\n",
      "Epoch:  2\n",
      "Loss:  1.0156176\n",
      "Epoch:  2\n",
      "Loss:  1.11406\n",
      "Epoch:  2\n",
      "Loss:  1.1487033\n",
      "Epoch:  2\n",
      "Loss:  1.1600446\n",
      "Epoch:  2\n",
      "Loss:  1.1131103\n",
      "Epoch:  2\n",
      "Loss:  1.0940248\n",
      "Epoch:  2\n",
      "Loss:  1.0614572\n",
      "Epoch:  2\n",
      "Loss:  1.078705\n",
      "Epoch:  2\n",
      "Loss:  1.1586354\n",
      "Epoch:  2\n",
      "Loss:  1.1721241\n",
      "Epoch:  2\n",
      "Loss:  1.1913561\n",
      "Epoch:  2\n",
      "Loss:  1.1126845\n",
      "Epoch:  2\n",
      "Loss:  1.2176853\n",
      "Epoch:  2\n",
      "Loss:  1.1036751\n",
      "Epoch:  2\n",
      "Loss:  1.1261054\n",
      "Epoch:  2\n",
      "Loss:  1.0980661\n",
      "Epoch:  2\n",
      "Loss:  1.1780665\n",
      "Epoch:  2\n",
      "Loss:  1.115878\n",
      "Epoch:  2\n",
      "Loss:  1.0661447\n",
      "Epoch:  2\n",
      "Loss:  1.0893303\n",
      "Epoch:  2\n",
      "Loss:  1.0076369\n",
      "Epoch:  2\n",
      "Loss:  1.0985827\n",
      "Epoch:  2\n",
      "Loss:  1.1857593\n",
      "Epoch:  2\n",
      "Loss:  1.1120133\n",
      "Epoch:  2\n",
      "Loss:  1.0670279\n",
      "Epoch:  2\n",
      "Loss:  1.1074855\n",
      "Epoch:  2\n",
      "Loss:  1.0085413\n",
      "Epoch:  2\n",
      "Loss:  1.0472882\n",
      "Epoch:  2\n",
      "Loss:  1.1073058\n",
      "Epoch:  2\n",
      "Loss:  1.1426097\n",
      "Epoch:  2\n",
      "Loss:  1.0246396\n",
      "Epoch:  2\n",
      "Loss:  1.1397146\n",
      "Epoch:  2\n",
      "Loss:  1.0574725\n",
      "Epoch:  2\n",
      "Loss:  1.0824068\n",
      "Epoch:  2\n",
      "Loss:  1.0842435\n",
      "Epoch:  2\n",
      "Loss:  1.0163939\n",
      "Epoch:  2\n",
      "Loss:  1.0925677\n",
      "Epoch:  2\n",
      "Loss:  1.1005538\n",
      "Epoch:  2\n",
      "Loss:  1.0919187\n",
      "Epoch:  2\n",
      "Loss:  1.0228989\n",
      "Epoch:  2\n",
      "Loss:  1.1810403\n",
      "Epoch:  2\n",
      "Loss:  1.1039629\n",
      "Epoch:  2\n",
      "Loss:  1.1772822\n",
      "Epoch:  2\n",
      "Loss:  0.9992503\n",
      "Epoch:  2\n",
      "Loss:  0.98762465\n",
      "Epoch:  2\n",
      "Loss:  1.1528783\n",
      "Epoch:  2\n",
      "Loss:  1.0353619\n",
      "Epoch:  2\n",
      "Loss:  1.1497462\n",
      "Epoch:  2\n",
      "Loss:  1.0809013\n",
      "Epoch:  2\n",
      "Loss:  0.9876083\n",
      "Epoch:  2\n",
      "Loss:  1.1368461\n",
      "Epoch:  2\n",
      "Loss:  1.148191\n",
      "Epoch:  2\n",
      "Loss:  1.054493\n",
      "Epoch:  2\n",
      "Loss:  1.0070059\n",
      "Epoch:  2\n",
      "Loss:  1.0327431\n",
      "Epoch:  3\n",
      "Loss:  0.92845905\n",
      "Epoch:  3\n",
      "Loss:  0.9353325\n",
      "Epoch:  3\n",
      "Loss:  0.9613358\n",
      "Epoch:  3\n",
      "Loss:  0.8920161\n",
      "Epoch:  3\n",
      "Loss:  0.9650305\n",
      "Epoch:  3\n",
      "Loss:  0.8783013\n",
      "Epoch:  3\n",
      "Loss:  0.9122049\n",
      "Epoch:  3\n",
      "Loss:  0.9827615\n",
      "Epoch:  3\n",
      "Loss:  0.9473241\n",
      "Epoch:  3\n",
      "Loss:  0.8804027\n",
      "Epoch:  3\n",
      "Loss:  0.90427285\n",
      "Epoch:  3\n",
      "Loss:  0.8586539\n",
      "Epoch:  3\n",
      "Loss:  0.9942028\n",
      "Epoch:  3\n",
      "Loss:  0.88329375\n",
      "Epoch:  3\n",
      "Loss:  0.80547714\n",
      "Epoch:  3\n",
      "Loss:  0.9211569\n",
      "Epoch:  3\n",
      "Loss:  0.8552027\n",
      "Epoch:  3\n",
      "Loss:  0.9068142\n",
      "Epoch:  3\n",
      "Loss:  1.0159096\n",
      "Epoch:  3\n",
      "Loss:  0.91802454\n",
      "Epoch:  3\n",
      "Loss:  0.81325495\n",
      "Epoch:  3\n",
      "Loss:  0.8993681\n",
      "Epoch:  3\n",
      "Loss:  0.8887159\n",
      "Epoch:  3\n",
      "Loss:  0.94079304\n",
      "Epoch:  3\n",
      "Loss:  0.8796892\n",
      "Epoch:  3\n",
      "Loss:  0.91154176\n",
      "Epoch:  3\n",
      "Loss:  0.9612567\n",
      "Epoch:  3\n",
      "Loss:  0.8685112\n",
      "Epoch:  3\n",
      "Loss:  1.0124215\n",
      "Epoch:  3\n",
      "Loss:  0.9060173\n",
      "Epoch:  3\n",
      "Loss:  0.8742293\n",
      "Epoch:  3\n",
      "Loss:  0.88787764\n",
      "Epoch:  3\n",
      "Loss:  0.9260337\n",
      "Epoch:  3\n",
      "Loss:  0.89489967\n",
      "Epoch:  3\n",
      "Loss:  1.037341\n",
      "Epoch:  3\n",
      "Loss:  0.9343482\n",
      "Epoch:  3\n",
      "Loss:  0.90059483\n",
      "Epoch:  3\n",
      "Loss:  0.8656104\n",
      "Epoch:  3\n",
      "Loss:  0.9011675\n",
      "Epoch:  3\n",
      "Loss:  0.97509336\n",
      "Epoch:  3\n",
      "Loss:  0.92732346\n",
      "Epoch:  3\n",
      "Loss:  0.934478\n",
      "Epoch:  3\n",
      "Loss:  0.88791436\n",
      "Epoch:  3\n",
      "Loss:  0.9822871\n",
      "Epoch:  3\n",
      "Loss:  0.91174173\n",
      "Epoch:  3\n",
      "Loss:  0.9140601\n",
      "Epoch:  3\n",
      "Loss:  0.9431194\n",
      "Epoch:  3\n",
      "Loss:  0.90942013\n",
      "Epoch:  3\n",
      "Loss:  0.8850625\n",
      "Epoch:  3\n",
      "Loss:  1.0246177\n",
      "Epoch:  3\n",
      "Loss:  0.9362243\n",
      "Epoch:  3\n",
      "Loss:  0.9321663\n",
      "Epoch:  3\n",
      "Loss:  1.0292109\n",
      "Epoch:  3\n",
      "Loss:  0.8957323\n",
      "Epoch:  3\n",
      "Loss:  1.0148593\n",
      "Epoch:  3\n",
      "Loss:  0.8247534\n",
      "Epoch:  3\n",
      "Loss:  0.8802191\n",
      "Epoch:  3\n",
      "Loss:  0.9119879\n",
      "Epoch:  3\n",
      "Loss:  0.8942766\n",
      "Epoch:  3\n",
      "Loss:  0.92812175\n",
      "Epoch:  3\n",
      "Loss:  0.9447309\n",
      "Epoch:  3\n",
      "Loss:  1.0109639\n",
      "Epoch:  3\n",
      "Loss:  0.86798906\n",
      "Epoch:  3\n",
      "Loss:  0.8907917\n",
      "Epoch:  3\n",
      "Loss:  0.84611034\n",
      "Epoch:  3\n",
      "Loss:  0.8982166\n",
      "Epoch:  3\n",
      "Loss:  0.8902362\n",
      "Epoch:  3\n",
      "Loss:  0.97630227\n",
      "Epoch:  3\n",
      "Loss:  0.9112848\n",
      "Epoch:  3\n",
      "Loss:  1.0563396\n",
      "Epoch:  3\n",
      "Loss:  0.885043\n",
      "Epoch:  3\n",
      "Loss:  1.0324415\n",
      "Epoch:  3\n",
      "Loss:  0.9375962\n",
      "Epoch:  3\n",
      "Loss:  0.9187034\n",
      "Epoch:  3\n",
      "Loss:  0.9288052\n",
      "Epoch:  3\n",
      "Loss:  0.9127542\n",
      "Epoch:  3\n",
      "Loss:  0.88183415\n",
      "Epoch:  3\n",
      "Loss:  0.99607295\n",
      "Epoch:  3\n",
      "Loss:  0.92989314\n",
      "Epoch:  3\n",
      "Loss:  0.8203146\n",
      "Epoch:  3\n",
      "Loss:  0.8567959\n",
      "Epoch:  3\n",
      "Loss:  0.9437873\n",
      "Epoch:  3\n",
      "Loss:  0.97929275\n",
      "Epoch:  3\n",
      "Loss:  1.0008637\n",
      "Epoch:  3\n",
      "Loss:  0.95282\n",
      "Epoch:  3\n",
      "Loss:  0.873162\n",
      "Epoch:  3\n",
      "Loss:  0.9275335\n",
      "Epoch:  3\n",
      "Loss:  0.96286976\n",
      "Epoch:  3\n",
      "Loss:  0.8964879\n",
      "Epoch:  3\n",
      "Loss:  0.92321455\n",
      "Epoch:  3\n",
      "Loss:  0.8055712\n",
      "Epoch:  3\n",
      "Loss:  0.9873086\n",
      "Epoch:  3\n",
      "Loss:  0.8904873\n",
      "Epoch:  3\n",
      "Loss:  0.8921995\n",
      "Epoch:  3\n",
      "Loss:  0.95497704\n",
      "Epoch:  3\n",
      "Loss:  0.95618963\n",
      "Epoch:  3\n",
      "Loss:  0.8002717\n",
      "Epoch:  3\n",
      "Loss:  0.9083718\n",
      "Epoch:  3\n",
      "Loss:  0.96999294\n",
      "Epoch:  3\n",
      "Loss:  0.85430175\n",
      "Epoch:  3\n",
      "Loss:  0.8930475\n",
      "Epoch:  3\n",
      "Loss:  0.9624425\n",
      "Epoch:  3\n",
      "Loss:  0.9105152\n",
      "Epoch:  3\n",
      "Loss:  0.8817588\n",
      "Epoch:  3\n",
      "Loss:  0.846196\n",
      "Epoch:  3\n",
      "Loss:  0.8385762\n",
      "Epoch:  3\n",
      "Loss:  0.99135315\n",
      "Epoch:  3\n",
      "Loss:  0.8874189\n",
      "Epoch:  3\n",
      "Loss:  0.9086431\n",
      "Epoch:  3\n",
      "Loss:  0.9040402\n",
      "Epoch:  3\n",
      "Loss:  0.84487855\n",
      "Epoch:  3\n",
      "Loss:  0.96523\n",
      "Epoch:  3\n",
      "Loss:  0.8732506\n",
      "Epoch:  3\n",
      "Loss:  0.92868316\n",
      "Epoch:  3\n",
      "Loss:  0.8976119\n",
      "Epoch:  3\n",
      "Loss:  0.88401306\n",
      "Epoch:  3\n",
      "Loss:  0.871243\n",
      "Epoch:  3\n",
      "Loss:  0.8954692\n",
      "Epoch:  3\n",
      "Loss:  0.8685589\n",
      "Epoch:  3\n",
      "Loss:  0.8961627\n",
      "Epoch:  3\n",
      "Loss:  0.91364706\n",
      "Epoch:  3\n",
      "Loss:  0.8469473\n",
      "Epoch:  3\n",
      "Loss:  0.97364587\n",
      "Epoch:  3\n",
      "Loss:  0.96592057\n",
      "Epoch:  3\n",
      "Loss:  0.83531123\n",
      "Epoch:  3\n",
      "Loss:  0.867792\n",
      "Epoch:  3\n",
      "Loss:  0.84163105\n",
      "Epoch:  3\n",
      "Loss:  0.92425174\n",
      "Epoch:  3\n",
      "Loss:  0.9527375\n",
      "Epoch:  3\n",
      "Loss:  0.8705462\n",
      "Epoch:  3\n",
      "Loss:  0.92086685\n",
      "Epoch:  3\n",
      "Loss:  0.82616407\n",
      "Epoch:  3\n",
      "Loss:  0.97185385\n",
      "Epoch:  3\n",
      "Loss:  0.7622071\n",
      "Epoch:  3\n",
      "Loss:  0.89869326\n",
      "Epoch:  3\n",
      "Loss:  0.8778175\n",
      "Epoch:  3\n",
      "Loss:  0.8904666\n",
      "Epoch:  3\n",
      "Loss:  0.7980943\n",
      "Epoch:  3\n",
      "Loss:  0.86856115\n",
      "Epoch:  3\n",
      "Loss:  0.9418232\n",
      "Epoch:  3\n",
      "Loss:  0.84783393\n",
      "Epoch:  3\n",
      "Loss:  0.795941\n",
      "Epoch:  3\n",
      "Loss:  0.9446348\n",
      "Epoch:  3\n",
      "Loss:  0.8043293\n",
      "Epoch:  3\n",
      "Loss:  0.90604764\n",
      "Epoch:  3\n",
      "Loss:  0.9239731\n",
      "Epoch:  3\n",
      "Loss:  0.99008673\n",
      "Epoch:  3\n",
      "Loss:  0.91930103\n",
      "Epoch:  3\n",
      "Loss:  0.93475896\n",
      "Epoch:  3\n",
      "Loss:  0.797047\n",
      "Epoch:  3\n",
      "Loss:  0.90294266\n",
      "Epoch:  3\n",
      "Loss:  0.7999774\n",
      "Epoch:  3\n",
      "Loss:  0.8922388\n",
      "Epoch:  3\n",
      "Loss:  0.86862355\n",
      "Epoch:  3\n",
      "Loss:  0.8873423\n",
      "Epoch:  3\n",
      "Loss:  0.9224326\n",
      "Epoch:  3\n",
      "Loss:  0.87190306\n",
      "Epoch:  3\n",
      "Loss:  0.88778275\n",
      "Epoch:  3\n",
      "Loss:  0.9608018\n",
      "Epoch:  3\n",
      "Loss:  1.0036767\n",
      "Epoch:  3\n",
      "Loss:  0.8849398\n",
      "Epoch:  3\n",
      "Loss:  0.88814753\n",
      "Epoch:  3\n",
      "Loss:  0.86685884\n",
      "Epoch:  3\n",
      "Loss:  0.82729274\n",
      "Epoch:  3\n",
      "Loss:  0.8713113\n",
      "Epoch:  3\n",
      "Loss:  0.9436801\n",
      "Epoch:  3\n",
      "Loss:  0.8179563\n",
      "Epoch:  3\n",
      "Loss:  0.88351583\n",
      "Epoch:  3\n",
      "Loss:  0.9290274\n",
      "Epoch:  3\n",
      "Loss:  0.95182467\n",
      "Epoch:  3\n",
      "Loss:  0.87598455\n",
      "Epoch:  3\n",
      "Loss:  0.9436269\n",
      "Epoch:  3\n",
      "Loss:  0.87286747\n",
      "Epoch:  3\n",
      "Loss:  0.93548095\n",
      "Epoch:  3\n",
      "Loss:  0.9396819\n",
      "Epoch:  3\n",
      "Loss:  0.9557172\n",
      "Epoch:  3\n",
      "Loss:  0.7736894\n",
      "Epoch:  3\n",
      "Loss:  0.8807988\n",
      "Epoch:  3\n",
      "Loss:  1.017702\n",
      "Epoch:  3\n",
      "Loss:  0.833855\n",
      "Epoch:  3\n",
      "Loss:  0.86454374\n",
      "Epoch:  3\n",
      "Loss:  0.8727335\n",
      "Epoch:  3\n",
      "Loss:  0.8357476\n",
      "Epoch:  3\n",
      "Loss:  0.898448\n",
      "Epoch:  3\n",
      "Loss:  0.90351164\n",
      "Epoch:  3\n",
      "Loss:  0.9073722\n",
      "Epoch:  3\n",
      "Loss:  0.9114609\n",
      "Epoch:  3\n",
      "Loss:  0.95629823\n",
      "Epoch:  3\n",
      "Loss:  0.91795796\n",
      "Epoch:  3\n",
      "Loss:  0.839851\n",
      "Epoch:  3\n",
      "Loss:  0.8640963\n",
      "Epoch:  3\n",
      "Loss:  0.88390285\n",
      "Epoch:  3\n",
      "Loss:  0.86560404\n",
      "Epoch:  3\n",
      "Loss:  0.8076951\n",
      "Epoch:  3\n",
      "Loss:  0.9881256\n",
      "Epoch:  3\n",
      "Loss:  0.8919964\n",
      "Epoch:  3\n",
      "Loss:  0.84491014\n",
      "Epoch:  3\n",
      "Loss:  0.9227625\n",
      "Epoch:  3\n",
      "Loss:  0.8935717\n",
      "Epoch:  3\n",
      "Loss:  0.84166753\n",
      "Epoch:  3\n",
      "Loss:  0.8454698\n",
      "Epoch:  3\n",
      "Loss:  0.88025904\n",
      "Epoch:  3\n",
      "Loss:  0.8587183\n",
      "Epoch:  3\n",
      "Loss:  0.8699001\n",
      "Epoch:  3\n",
      "Loss:  0.92666465\n",
      "Epoch:  3\n",
      "Loss:  0.78293496\n",
      "Epoch:  3\n",
      "Loss:  0.8471114\n",
      "Epoch:  3\n",
      "Loss:  0.8196923\n",
      "Epoch:  3\n",
      "Loss:  0.8981082\n",
      "Epoch:  3\n",
      "Loss:  0.95310915\n",
      "Epoch:  3\n",
      "Loss:  0.7863989\n",
      "Epoch:  3\n",
      "Loss:  0.89234227\n",
      "Epoch:  3\n",
      "Loss:  0.8553119\n",
      "Epoch:  3\n",
      "Loss:  0.99776953\n",
      "Epoch:  3\n",
      "Loss:  0.8897294\n",
      "Epoch:  3\n",
      "Loss:  0.8726238\n",
      "Epoch:  3\n",
      "Loss:  0.92565596\n",
      "Epoch:  3\n",
      "Loss:  0.8730448\n",
      "Epoch:  3\n",
      "Loss:  0.9007943\n",
      "Epoch:  3\n",
      "Loss:  0.915316\n",
      "Epoch:  3\n",
      "Loss:  0.83010757\n",
      "Epoch:  3\n",
      "Loss:  0.9144496\n",
      "Epoch:  3\n",
      "Loss:  0.866763\n",
      "Epoch:  3\n",
      "Loss:  0.85299367\n",
      "Epoch:  3\n",
      "Loss:  0.80741006\n",
      "Epoch:  3\n",
      "Loss:  0.7847787\n",
      "Epoch:  3\n",
      "Loss:  0.8089983\n",
      "Epoch:  3\n",
      "Loss:  0.83200264\n",
      "Epoch:  3\n",
      "Loss:  0.8320674\n",
      "Epoch:  3\n",
      "Loss:  0.96390474\n",
      "Epoch:  3\n",
      "Loss:  0.85773957\n",
      "Epoch:  3\n",
      "Loss:  0.89608383\n",
      "Epoch:  3\n",
      "Loss:  0.8935172\n",
      "Epoch:  3\n",
      "Loss:  0.91490096\n",
      "Epoch:  3\n",
      "Loss:  0.9425111\n",
      "Epoch:  3\n",
      "Loss:  0.8913022\n",
      "Epoch:  3\n",
      "Loss:  0.8360755\n",
      "Epoch:  3\n",
      "Loss:  0.7844043\n",
      "Epoch:  3\n",
      "Loss:  0.8995941\n",
      "Epoch:  3\n",
      "Loss:  0.8538071\n",
      "Epoch:  3\n",
      "Loss:  0.853731\n",
      "Epoch:  3\n",
      "Loss:  0.8916153\n",
      "Epoch:  3\n",
      "Loss:  0.8778041\n",
      "Epoch:  3\n",
      "Loss:  0.76624763\n",
      "Epoch:  3\n",
      "Loss:  0.8749608\n",
      "Epoch:  3\n",
      "Loss:  0.8156501\n",
      "Epoch:  3\n",
      "Loss:  0.891102\n",
      "Epoch:  3\n",
      "Loss:  0.8934194\n",
      "Epoch:  3\n",
      "Loss:  0.91871595\n",
      "Epoch:  3\n",
      "Loss:  0.8715598\n",
      "Epoch:  4\n",
      "Loss:  0.64068335\n",
      "Epoch:  4\n",
      "Loss:  0.6238764\n",
      "Epoch:  4\n",
      "Loss:  0.7946727\n",
      "Epoch:  4\n",
      "Loss:  0.6004824\n",
      "Epoch:  4\n",
      "Loss:  0.5823967\n",
      "Epoch:  4\n",
      "Loss:  0.60888135\n",
      "Epoch:  4\n",
      "Loss:  0.70720947\n",
      "Epoch:  4\n",
      "Loss:  0.74903685\n",
      "Epoch:  4\n",
      "Loss:  0.60847676\n",
      "Epoch:  4\n",
      "Loss:  0.6264485\n",
      "Epoch:  4\n",
      "Loss:  0.67946726\n",
      "Epoch:  4\n",
      "Loss:  0.6563533\n",
      "Epoch:  4\n",
      "Loss:  0.700011\n",
      "Epoch:  4\n",
      "Loss:  0.6848999\n",
      "Epoch:  4\n",
      "Loss:  0.62534994\n",
      "Epoch:  4\n",
      "Loss:  0.63207227\n",
      "Epoch:  4\n",
      "Loss:  0.6476375\n",
      "Epoch:  4\n",
      "Loss:  0.749318\n",
      "Epoch:  4\n",
      "Loss:  0.64740217\n",
      "Epoch:  4\n",
      "Loss:  0.62311584\n",
      "Epoch:  4\n",
      "Loss:  0.7263829\n",
      "Epoch:  4\n",
      "Loss:  0.7173166\n",
      "Epoch:  4\n",
      "Loss:  0.6091533\n",
      "Epoch:  4\n",
      "Loss:  0.65981156\n",
      "Epoch:  4\n",
      "Loss:  0.6475961\n",
      "Epoch:  4\n",
      "Loss:  0.6606465\n",
      "Epoch:  4\n",
      "Loss:  0.6870548\n",
      "Epoch:  4\n",
      "Loss:  0.6589643\n",
      "Epoch:  4\n",
      "Loss:  0.7065013\n",
      "Epoch:  4\n",
      "Loss:  0.6275023\n",
      "Epoch:  4\n",
      "Loss:  0.58939636\n",
      "Epoch:  4\n",
      "Loss:  0.6550251\n",
      "Epoch:  4\n",
      "Loss:  0.6684009\n",
      "Epoch:  4\n",
      "Loss:  0.7031649\n",
      "Epoch:  4\n",
      "Loss:  0.7071991\n",
      "Epoch:  4\n",
      "Loss:  0.6361761\n",
      "Epoch:  4\n",
      "Loss:  0.72322243\n",
      "Epoch:  4\n",
      "Loss:  0.6393226\n",
      "Epoch:  4\n",
      "Loss:  0.6370808\n",
      "Epoch:  4\n",
      "Loss:  0.5952648\n",
      "Epoch:  4\n",
      "Loss:  0.67045605\n",
      "Epoch:  4\n",
      "Loss:  0.65212387\n",
      "Epoch:  4\n",
      "Loss:  0.68593\n",
      "Epoch:  4\n",
      "Loss:  0.7285737\n",
      "Epoch:  4\n",
      "Loss:  0.6479451\n",
      "Epoch:  4\n",
      "Loss:  0.6745119\n",
      "Epoch:  4\n",
      "Loss:  0.6115674\n",
      "Epoch:  4\n",
      "Loss:  0.6584278\n",
      "Epoch:  4\n",
      "Loss:  0.71062815\n",
      "Epoch:  4\n",
      "Loss:  0.73645526\n",
      "Epoch:  4\n",
      "Loss:  0.6712602\n",
      "Epoch:  4\n",
      "Loss:  0.6950887\n",
      "Epoch:  4\n",
      "Loss:  0.7196589\n",
      "Epoch:  4\n",
      "Loss:  0.6408639\n",
      "Epoch:  4\n",
      "Loss:  0.796944\n",
      "Epoch:  4\n",
      "Loss:  0.74385846\n",
      "Epoch:  4\n",
      "Loss:  0.6935791\n",
      "Epoch:  4\n",
      "Loss:  0.65597993\n",
      "Epoch:  4\n",
      "Loss:  0.7649513\n",
      "Epoch:  4\n",
      "Loss:  0.66795\n",
      "Epoch:  4\n",
      "Loss:  0.6559304\n",
      "Epoch:  4\n",
      "Loss:  0.6148561\n",
      "Epoch:  4\n",
      "Loss:  0.7780313\n",
      "Epoch:  4\n",
      "Loss:  0.6727581\n",
      "Epoch:  4\n",
      "Loss:  0.6357029\n",
      "Epoch:  4\n",
      "Loss:  0.6021856\n",
      "Epoch:  4\n",
      "Loss:  0.5765238\n",
      "Epoch:  4\n",
      "Loss:  0.7389177\n",
      "Epoch:  4\n",
      "Loss:  0.7796111\n",
      "Epoch:  4\n",
      "Loss:  0.6845272\n",
      "Epoch:  4\n",
      "Loss:  0.69723946\n",
      "Epoch:  4\n",
      "Loss:  0.6523972\n",
      "Epoch:  4\n",
      "Loss:  0.678941\n",
      "Epoch:  4\n",
      "Loss:  0.7019007\n",
      "Epoch:  4\n",
      "Loss:  0.62122643\n",
      "Epoch:  4\n",
      "Loss:  0.68873674\n",
      "Epoch:  4\n",
      "Loss:  0.68941003\n",
      "Epoch:  4\n",
      "Loss:  0.6363147\n",
      "Epoch:  4\n",
      "Loss:  0.7181701\n",
      "Epoch:  4\n",
      "Loss:  0.6685126\n",
      "Epoch:  4\n",
      "Loss:  0.65433145\n",
      "Epoch:  4\n",
      "Loss:  0.7155277\n",
      "Epoch:  4\n",
      "Loss:  0.6286336\n",
      "Epoch:  4\n",
      "Loss:  0.6582133\n",
      "Epoch:  4\n",
      "Loss:  0.68376005\n",
      "Epoch:  4\n",
      "Loss:  0.66605115\n",
      "Epoch:  4\n",
      "Loss:  0.7297415\n",
      "Epoch:  4\n",
      "Loss:  0.6559989\n",
      "Epoch:  4\n",
      "Loss:  0.7157017\n",
      "Epoch:  4\n",
      "Loss:  0.6585672\n",
      "Epoch:  4\n",
      "Loss:  0.7076504\n",
      "Epoch:  4\n",
      "Loss:  0.67652285\n",
      "Epoch:  4\n",
      "Loss:  0.5987595\n",
      "Epoch:  4\n",
      "Loss:  0.68448985\n",
      "Epoch:  4\n",
      "Loss:  0.67769605\n",
      "Epoch:  4\n",
      "Loss:  0.6733313\n",
      "Epoch:  4\n",
      "Loss:  0.72830355\n",
      "Epoch:  4\n",
      "Loss:  0.6466957\n",
      "Epoch:  4\n",
      "Loss:  0.67712045\n",
      "Epoch:  4\n",
      "Loss:  0.68467677\n",
      "Epoch:  4\n",
      "Loss:  0.73258656\n",
      "Epoch:  4\n",
      "Loss:  0.6595065\n",
      "Epoch:  4\n",
      "Loss:  0.730464\n",
      "Epoch:  4\n",
      "Loss:  0.78253937\n",
      "Epoch:  4\n",
      "Loss:  0.78089386\n",
      "Epoch:  4\n",
      "Loss:  0.7435111\n",
      "Epoch:  4\n",
      "Loss:  0.583127\n",
      "Epoch:  4\n",
      "Loss:  0.6511772\n",
      "Epoch:  4\n",
      "Loss:  0.6458813\n",
      "Epoch:  4\n",
      "Loss:  0.7638782\n",
      "Epoch:  4\n",
      "Loss:  0.6910094\n",
      "Epoch:  4\n",
      "Loss:  0.69728655\n",
      "Epoch:  4\n",
      "Loss:  0.7189349\n",
      "Epoch:  4\n",
      "Loss:  0.8259637\n",
      "Epoch:  4\n",
      "Loss:  0.6311327\n",
      "Epoch:  4\n",
      "Loss:  0.6220792\n",
      "Epoch:  4\n",
      "Loss:  0.608981\n",
      "Epoch:  4\n",
      "Loss:  0.72998524\n",
      "Epoch:  4\n",
      "Loss:  0.64332336\n",
      "Epoch:  4\n",
      "Loss:  0.7503861\n",
      "Epoch:  4\n",
      "Loss:  0.8308717\n",
      "Epoch:  4\n",
      "Loss:  0.7049721\n",
      "Epoch:  4\n",
      "Loss:  0.7303649\n",
      "Epoch:  4\n",
      "Loss:  0.6985274\n",
      "Epoch:  4\n",
      "Loss:  0.71192807\n",
      "Epoch:  4\n",
      "Loss:  0.68921506\n",
      "Epoch:  4\n",
      "Loss:  0.6855599\n",
      "Epoch:  4\n",
      "Loss:  0.72179735\n",
      "Epoch:  4\n",
      "Loss:  0.6151002\n",
      "Epoch:  4\n",
      "Loss:  0.7206868\n",
      "Epoch:  4\n",
      "Loss:  0.7174314\n",
      "Epoch:  4\n",
      "Loss:  0.6361617\n",
      "Epoch:  4\n",
      "Loss:  0.694928\n",
      "Epoch:  4\n",
      "Loss:  0.65816987\n",
      "Epoch:  4\n",
      "Loss:  0.6945986\n",
      "Epoch:  4\n",
      "Loss:  0.66130126\n",
      "Epoch:  4\n",
      "Loss:  0.7193405\n",
      "Epoch:  4\n",
      "Loss:  0.5584485\n",
      "Epoch:  4\n",
      "Loss:  0.6288628\n",
      "Epoch:  4\n",
      "Loss:  0.61176026\n",
      "Epoch:  4\n",
      "Loss:  0.643301\n",
      "Epoch:  4\n",
      "Loss:  0.7168778\n",
      "Epoch:  4\n",
      "Loss:  0.6058904\n",
      "Epoch:  4\n",
      "Loss:  0.7045612\n",
      "Epoch:  4\n",
      "Loss:  0.6189047\n",
      "Epoch:  4\n",
      "Loss:  0.65539485\n",
      "Epoch:  4\n",
      "Loss:  0.6246233\n",
      "Epoch:  4\n",
      "Loss:  0.78403926\n",
      "Epoch:  4\n",
      "Loss:  0.5645108\n",
      "Epoch:  4\n",
      "Loss:  0.68099344\n",
      "Epoch:  4\n",
      "Loss:  0.6601361\n",
      "Epoch:  4\n",
      "Loss:  0.60056365\n",
      "Epoch:  4\n",
      "Loss:  0.70048004\n",
      "Epoch:  4\n",
      "Loss:  0.7060401\n",
      "Epoch:  4\n",
      "Loss:  0.71367973\n",
      "Epoch:  4\n",
      "Loss:  0.64512354\n",
      "Epoch:  4\n",
      "Loss:  0.73617923\n",
      "Epoch:  4\n",
      "Loss:  0.677266\n",
      "Epoch:  4\n",
      "Loss:  0.6460714\n",
      "Epoch:  4\n",
      "Loss:  0.64056027\n",
      "Epoch:  4\n",
      "Loss:  0.6749207\n",
      "Epoch:  4\n",
      "Loss:  0.6987492\n",
      "Epoch:  4\n",
      "Loss:  0.73908895\n",
      "Epoch:  4\n",
      "Loss:  0.65416485\n",
      "Epoch:  4\n",
      "Loss:  0.6087724\n",
      "Epoch:  4\n",
      "Loss:  0.65164727\n",
      "Epoch:  4\n",
      "Loss:  0.67422277\n",
      "Epoch:  4\n",
      "Loss:  0.7086499\n",
      "Epoch:  4\n",
      "Loss:  0.69404906\n",
      "Epoch:  4\n",
      "Loss:  0.6038748\n",
      "Epoch:  4\n",
      "Loss:  0.6433882\n",
      "Epoch:  4\n",
      "Loss:  0.70840406\n",
      "Epoch:  4\n",
      "Loss:  0.6581688\n",
      "Epoch:  4\n",
      "Loss:  0.6710584\n",
      "Epoch:  4\n",
      "Loss:  0.6679538\n",
      "Epoch:  4\n",
      "Loss:  0.7012416\n",
      "Epoch:  4\n",
      "Loss:  0.68083584\n",
      "Epoch:  4\n",
      "Loss:  0.6661695\n",
      "Epoch:  4\n",
      "Loss:  0.7012565\n",
      "Epoch:  4\n",
      "Loss:  0.6459787\n",
      "Epoch:  4\n",
      "Loss:  0.69817287\n",
      "Epoch:  4\n",
      "Loss:  0.55769163\n",
      "Epoch:  4\n",
      "Loss:  0.5949391\n",
      "Epoch:  4\n",
      "Loss:  0.68518317\n",
      "Epoch:  4\n",
      "Loss:  0.7637063\n",
      "Epoch:  4\n",
      "Loss:  0.65615743\n",
      "Epoch:  4\n",
      "Loss:  0.67722535\n",
      "Epoch:  4\n",
      "Loss:  0.6386493\n",
      "Epoch:  4\n",
      "Loss:  0.7391013\n",
      "Epoch:  4\n",
      "Loss:  0.69029135\n",
      "Epoch:  4\n",
      "Loss:  0.60692686\n",
      "Epoch:  4\n",
      "Loss:  0.71554667\n",
      "Epoch:  4\n",
      "Loss:  0.6900782\n",
      "Epoch:  4\n",
      "Loss:  0.6580667\n",
      "Epoch:  4\n",
      "Loss:  0.64828706\n",
      "Epoch:  4\n",
      "Loss:  0.72714204\n",
      "Epoch:  4\n",
      "Loss:  0.657534\n",
      "Epoch:  4\n",
      "Loss:  0.8002027\n",
      "Epoch:  4\n",
      "Loss:  0.6300366\n",
      "Epoch:  4\n",
      "Loss:  0.6314605\n",
      "Epoch:  4\n",
      "Loss:  0.64536643\n",
      "Epoch:  4\n",
      "Loss:  0.7045227\n",
      "Epoch:  4\n",
      "Loss:  0.5746206\n",
      "Epoch:  4\n",
      "Loss:  0.71264505\n",
      "Epoch:  4\n",
      "Loss:  0.67228854\n",
      "Epoch:  4\n",
      "Loss:  0.71248394\n",
      "Epoch:  4\n",
      "Loss:  0.61966133\n",
      "Epoch:  4\n",
      "Loss:  0.68898654\n",
      "Epoch:  4\n",
      "Loss:  0.6311587\n",
      "Epoch:  4\n",
      "Loss:  0.6458498\n",
      "Epoch:  4\n",
      "Loss:  0.69130075\n",
      "Epoch:  4\n",
      "Loss:  0.71533304\n",
      "Epoch:  4\n",
      "Loss:  0.7247391\n",
      "Epoch:  4\n",
      "Loss:  0.6220163\n",
      "Epoch:  4\n",
      "Loss:  0.6954652\n",
      "Epoch:  4\n",
      "Loss:  0.6667176\n",
      "Epoch:  4\n",
      "Loss:  0.64698803\n",
      "Epoch:  4\n",
      "Loss:  0.59627175\n",
      "Epoch:  4\n",
      "Loss:  0.6694261\n",
      "Epoch:  4\n",
      "Loss:  0.6764798\n",
      "Epoch:  4\n",
      "Loss:  0.73380333\n",
      "Epoch:  4\n",
      "Loss:  0.635047\n",
      "Epoch:  4\n",
      "Loss:  0.73059034\n",
      "Epoch:  4\n",
      "Loss:  0.7199441\n",
      "Epoch:  4\n",
      "Loss:  0.60864955\n",
      "Epoch:  4\n",
      "Loss:  0.6366706\n",
      "Epoch:  4\n",
      "Loss:  0.72686774\n",
      "Epoch:  4\n",
      "Loss:  0.6805881\n",
      "Epoch:  4\n",
      "Loss:  0.66781956\n",
      "Epoch:  4\n",
      "Loss:  0.7672652\n",
      "Epoch:  4\n",
      "Loss:  0.66297007\n",
      "Epoch:  4\n",
      "Loss:  0.6957357\n",
      "Epoch:  4\n",
      "Loss:  0.6916859\n",
      "Epoch:  4\n",
      "Loss:  0.6904651\n",
      "Epoch:  4\n",
      "Loss:  0.6027103\n",
      "Epoch:  4\n",
      "Loss:  0.7838282\n",
      "Epoch:  4\n",
      "Loss:  0.746748\n",
      "Epoch:  4\n",
      "Loss:  0.6821989\n",
      "Epoch:  4\n",
      "Loss:  0.60009897\n",
      "Epoch:  4\n",
      "Loss:  0.6852709\n",
      "Epoch:  4\n",
      "Loss:  0.6825864\n",
      "Epoch:  4\n",
      "Loss:  0.64824474\n",
      "Epoch:  4\n",
      "Loss:  0.7032997\n",
      "Epoch:  4\n",
      "Loss:  0.68177277\n",
      "Epoch:  4\n",
      "Loss:  0.6437867\n",
      "Epoch:  4\n",
      "Loss:  0.6752237\n",
      "Epoch:  4\n",
      "Loss:  0.62834156\n",
      "Epoch:  4\n",
      "Loss:  0.6731611\n",
      "Epoch:  4\n",
      "Loss:  0.64792335\n",
      "Epoch:  4\n",
      "Loss:  0.6491937\n",
      "Epoch:  5\n",
      "Loss:  0.48625502\n",
      "Epoch:  5\n",
      "Loss:  0.47305137\n",
      "Epoch:  5\n",
      "Loss:  0.40562636\n",
      "Epoch:  5\n",
      "Loss:  0.45107874\n",
      "Epoch:  5\n",
      "Loss:  0.43973333\n",
      "Epoch:  5\n",
      "Loss:  0.42112646\n",
      "Epoch:  5\n",
      "Loss:  0.44176126\n",
      "Epoch:  5\n",
      "Loss:  0.45182896\n",
      "Epoch:  5\n",
      "Loss:  0.49233857\n",
      "Epoch:  5\n",
      "Loss:  0.49503103\n",
      "Epoch:  5\n",
      "Loss:  0.42132258\n",
      "Epoch:  5\n",
      "Loss:  0.47087988\n",
      "Epoch:  5\n",
      "Loss:  0.44522852\n",
      "Epoch:  5\n",
      "Loss:  0.49474192\n",
      "Epoch:  5\n",
      "Loss:  0.49271291\n",
      "Epoch:  5\n",
      "Loss:  0.43837005\n",
      "Epoch:  5\n",
      "Loss:  0.5269935\n",
      "Epoch:  5\n",
      "Loss:  0.46341878\n",
      "Epoch:  5\n",
      "Loss:  0.46279573\n",
      "Epoch:  5\n",
      "Loss:  0.47313628\n",
      "Epoch:  5\n",
      "Loss:  0.4310279\n",
      "Epoch:  5\n",
      "Loss:  0.38397262\n",
      "Epoch:  5\n",
      "Loss:  0.45275483\n",
      "Epoch:  5\n",
      "Loss:  0.44765368\n",
      "Epoch:  5\n",
      "Loss:  0.49165836\n",
      "Epoch:  5\n",
      "Loss:  0.4329629\n",
      "Epoch:  5\n",
      "Loss:  0.44024697\n",
      "Epoch:  5\n",
      "Loss:  0.4939835\n",
      "Epoch:  5\n",
      "Loss:  0.45023006\n",
      "Epoch:  5\n",
      "Loss:  0.42559743\n",
      "Epoch:  5\n",
      "Loss:  0.40096068\n",
      "Epoch:  5\n",
      "Loss:  0.4632113\n",
      "Epoch:  5\n",
      "Loss:  0.47128636\n",
      "Epoch:  5\n",
      "Loss:  0.4764471\n",
      "Epoch:  5\n",
      "Loss:  0.3987569\n",
      "Epoch:  5\n",
      "Loss:  0.44041675\n",
      "Epoch:  5\n",
      "Loss:  0.43405503\n",
      "Epoch:  5\n",
      "Loss:  0.50750816\n",
      "Epoch:  5\n",
      "Loss:  0.43884888\n",
      "Epoch:  5\n",
      "Loss:  0.54729074\n",
      "Epoch:  5\n",
      "Loss:  0.48114976\n",
      "Epoch:  5\n",
      "Loss:  0.53629863\n",
      "Epoch:  5\n",
      "Loss:  0.44985765\n",
      "Epoch:  5\n",
      "Loss:  0.47622672\n",
      "Epoch:  5\n",
      "Loss:  0.46283403\n",
      "Epoch:  5\n",
      "Loss:  0.41842747\n",
      "Epoch:  5\n",
      "Loss:  0.45904112\n",
      "Epoch:  5\n",
      "Loss:  0.5072917\n",
      "Epoch:  5\n",
      "Loss:  0.40959486\n",
      "Epoch:  5\n",
      "Loss:  0.4135603\n",
      "Epoch:  5\n",
      "Loss:  0.40228406\n",
      "Epoch:  5\n",
      "Loss:  0.4618195\n",
      "Epoch:  5\n",
      "Loss:  0.52279603\n",
      "Epoch:  5\n",
      "Loss:  0.49651217\n",
      "Epoch:  5\n",
      "Loss:  0.517036\n",
      "Epoch:  5\n",
      "Loss:  0.4341138\n",
      "Epoch:  5\n",
      "Loss:  0.4461878\n",
      "Epoch:  5\n",
      "Loss:  0.45570207\n",
      "Epoch:  5\n",
      "Loss:  0.48119158\n",
      "Epoch:  5\n",
      "Loss:  0.5044184\n",
      "Epoch:  5\n",
      "Loss:  0.39418307\n",
      "Epoch:  5\n",
      "Loss:  0.42453343\n",
      "Epoch:  5\n",
      "Loss:  0.46295872\n",
      "Epoch:  5\n",
      "Loss:  0.43225694\n",
      "Epoch:  5\n",
      "Loss:  0.51984596\n",
      "Epoch:  5\n",
      "Loss:  0.42080078\n",
      "Epoch:  5\n",
      "Loss:  0.4746198\n",
      "Epoch:  5\n",
      "Loss:  0.51872206\n",
      "Epoch:  5\n",
      "Loss:  0.5115411\n",
      "Epoch:  5\n",
      "Loss:  0.42923298\n",
      "Epoch:  5\n",
      "Loss:  0.44751358\n",
      "Epoch:  5\n",
      "Loss:  0.46507248\n",
      "Epoch:  5\n",
      "Loss:  0.5315624\n",
      "Epoch:  5\n",
      "Loss:  0.4344038\n",
      "Epoch:  5\n",
      "Loss:  0.53841406\n",
      "Epoch:  5\n",
      "Loss:  0.4384099\n",
      "Epoch:  5\n",
      "Loss:  0.39632425\n",
      "Epoch:  5\n",
      "Loss:  0.39882526\n",
      "Epoch:  5\n",
      "Loss:  0.4939383\n",
      "Epoch:  5\n",
      "Loss:  0.43968725\n",
      "Epoch:  5\n",
      "Loss:  0.47714344\n",
      "Epoch:  5\n",
      "Loss:  0.46358103\n",
      "Epoch:  5\n",
      "Loss:  0.42625195\n",
      "Epoch:  5\n",
      "Loss:  0.50114787\n",
      "Epoch:  5\n",
      "Loss:  0.4222339\n",
      "Epoch:  5\n",
      "Loss:  0.42807484\n",
      "Epoch:  5\n",
      "Loss:  0.49552804\n",
      "Epoch:  5\n",
      "Loss:  0.49573588\n",
      "Epoch:  5\n",
      "Loss:  0.51628375\n",
      "Epoch:  5\n",
      "Loss:  0.54525673\n",
      "Epoch:  5\n",
      "Loss:  0.4699162\n",
      "Epoch:  5\n",
      "Loss:  0.41443712\n",
      "Epoch:  5\n",
      "Loss:  0.46043754\n",
      "Epoch:  5\n",
      "Loss:  0.5107824\n",
      "Epoch:  5\n",
      "Loss:  0.5081614\n",
      "Epoch:  5\n",
      "Loss:  0.4438541\n",
      "Epoch:  5\n",
      "Loss:  0.47173566\n",
      "Epoch:  5\n",
      "Loss:  0.48664936\n",
      "Epoch:  5\n",
      "Loss:  0.41081253\n",
      "Epoch:  5\n",
      "Loss:  0.45456797\n",
      "Epoch:  5\n",
      "Loss:  0.5122536\n",
      "Epoch:  5\n",
      "Loss:  0.505752\n",
      "Epoch:  5\n",
      "Loss:  0.51426154\n",
      "Epoch:  5\n",
      "Loss:  0.491927\n",
      "Epoch:  5\n",
      "Loss:  0.5167396\n",
      "Epoch:  5\n",
      "Loss:  0.48870888\n",
      "Epoch:  5\n",
      "Loss:  0.45100266\n",
      "Epoch:  5\n",
      "Loss:  0.52870846\n",
      "Epoch:  5\n",
      "Loss:  0.43639264\n",
      "Epoch:  5\n",
      "Loss:  0.560029\n",
      "Epoch:  5\n",
      "Loss:  0.47137052\n",
      "Epoch:  5\n",
      "Loss:  0.49572748\n",
      "Epoch:  5\n",
      "Loss:  0.54817003\n",
      "Epoch:  5\n",
      "Loss:  0.41104594\n",
      "Epoch:  5\n",
      "Loss:  0.4623386\n",
      "Epoch:  5\n",
      "Loss:  0.44215408\n",
      "Epoch:  5\n",
      "Loss:  0.48147058\n",
      "Epoch:  5\n",
      "Loss:  0.53476346\n",
      "Epoch:  5\n",
      "Loss:  0.4325327\n",
      "Epoch:  5\n",
      "Loss:  0.5098705\n",
      "Epoch:  5\n",
      "Loss:  0.51662505\n",
      "Epoch:  5\n",
      "Loss:  0.48995614\n",
      "Epoch:  5\n",
      "Loss:  0.4093031\n",
      "Epoch:  5\n",
      "Loss:  0.49034533\n",
      "Epoch:  5\n",
      "Loss:  0.5068289\n",
      "Epoch:  5\n",
      "Loss:  0.50088954\n",
      "Epoch:  5\n",
      "Loss:  0.5563653\n",
      "Epoch:  5\n",
      "Loss:  0.43227062\n",
      "Epoch:  5\n",
      "Loss:  0.47383928\n",
      "Epoch:  5\n",
      "Loss:  0.5227643\n",
      "Epoch:  5\n",
      "Loss:  0.46340743\n",
      "Epoch:  5\n",
      "Loss:  0.4820327\n",
      "Epoch:  5\n",
      "Loss:  0.52352035\n",
      "Epoch:  5\n",
      "Loss:  0.54828644\n",
      "Epoch:  5\n",
      "Loss:  0.50119245\n",
      "Epoch:  5\n",
      "Loss:  0.48602137\n",
      "Epoch:  5\n",
      "Loss:  0.49738544\n",
      "Epoch:  5\n",
      "Loss:  0.43926054\n",
      "Epoch:  5\n",
      "Loss:  0.52460515\n",
      "Epoch:  5\n",
      "Loss:  0.4297556\n",
      "Epoch:  5\n",
      "Loss:  0.5000633\n",
      "Epoch:  5\n",
      "Loss:  0.5033812\n",
      "Epoch:  5\n",
      "Loss:  0.5927783\n",
      "Epoch:  5\n",
      "Loss:  0.47751054\n",
      "Epoch:  5\n",
      "Loss:  0.520825\n",
      "Epoch:  5\n",
      "Loss:  0.48958296\n",
      "Epoch:  5\n",
      "Loss:  0.46571583\n",
      "Epoch:  5\n",
      "Loss:  0.4792541\n",
      "Epoch:  5\n",
      "Loss:  0.43313545\n",
      "Epoch:  5\n",
      "Loss:  0.5878595\n",
      "Epoch:  5\n",
      "Loss:  0.44490567\n",
      "Epoch:  5\n",
      "Loss:  0.49919456\n",
      "Epoch:  5\n",
      "Loss:  0.5542229\n",
      "Epoch:  5\n",
      "Loss:  0.54155463\n",
      "Epoch:  5\n",
      "Loss:  0.45039082\n",
      "Epoch:  5\n",
      "Loss:  0.5434628\n",
      "Epoch:  5\n",
      "Loss:  0.50321496\n",
      "Epoch:  5\n",
      "Loss:  0.50493056\n",
      "Epoch:  5\n",
      "Loss:  0.49085206\n",
      "Epoch:  5\n",
      "Loss:  0.49321342\n",
      "Epoch:  5\n",
      "Loss:  0.55980456\n",
      "Epoch:  5\n",
      "Loss:  0.49091452\n",
      "Epoch:  5\n",
      "Loss:  0.44980788\n",
      "Epoch:  5\n",
      "Loss:  0.41638923\n",
      "Epoch:  5\n",
      "Loss:  0.49242693\n",
      "Epoch:  5\n",
      "Loss:  0.5057981\n",
      "Epoch:  5\n",
      "Loss:  0.46719283\n",
      "Epoch:  5\n",
      "Loss:  0.5477311\n",
      "Epoch:  5\n",
      "Loss:  0.5161643\n",
      "Epoch:  5\n",
      "Loss:  0.57700413\n",
      "Epoch:  5\n",
      "Loss:  0.4919466\n",
      "Epoch:  5\n",
      "Loss:  0.45919505\n",
      "Epoch:  5\n",
      "Loss:  0.47207385\n",
      "Epoch:  5\n",
      "Loss:  0.47753716\n",
      "Epoch:  5\n",
      "Loss:  0.5658689\n",
      "Epoch:  5\n",
      "Loss:  0.45189038\n",
      "Epoch:  5\n",
      "Loss:  0.4728833\n",
      "Epoch:  5\n",
      "Loss:  0.52618676\n",
      "Epoch:  5\n",
      "Loss:  0.44307834\n",
      "Epoch:  5\n",
      "Loss:  0.5165376\n",
      "Epoch:  5\n",
      "Loss:  0.4375034\n",
      "Epoch:  5\n",
      "Loss:  0.5218221\n",
      "Epoch:  5\n",
      "Loss:  0.4653837\n",
      "Epoch:  5\n",
      "Loss:  0.52047366\n",
      "Epoch:  5\n",
      "Loss:  0.54396015\n",
      "Epoch:  5\n",
      "Loss:  0.42180133\n",
      "Epoch:  5\n",
      "Loss:  0.53215826\n",
      "Epoch:  5\n",
      "Loss:  0.4454319\n",
      "Epoch:  5\n",
      "Loss:  0.50423443\n",
      "Epoch:  5\n",
      "Loss:  0.5421289\n",
      "Epoch:  5\n",
      "Loss:  0.570019\n",
      "Epoch:  5\n",
      "Loss:  0.51089656\n",
      "Epoch:  5\n",
      "Loss:  0.49169248\n",
      "Epoch:  5\n",
      "Loss:  0.4442144\n",
      "Epoch:  5\n",
      "Loss:  0.5232664\n",
      "Epoch:  5\n",
      "Loss:  0.5854166\n",
      "Epoch:  5\n",
      "Loss:  0.5414385\n",
      "Epoch:  5\n",
      "Loss:  0.51248103\n",
      "Epoch:  5\n",
      "Loss:  0.44848123\n",
      "Epoch:  5\n",
      "Loss:  0.53715986\n",
      "Epoch:  5\n",
      "Loss:  0.5433426\n",
      "Epoch:  5\n",
      "Loss:  0.52980465\n",
      "Epoch:  5\n",
      "Loss:  0.5247044\n",
      "Epoch:  5\n",
      "Loss:  0.43645263\n",
      "Epoch:  5\n",
      "Loss:  0.4445499\n",
      "Epoch:  5\n",
      "Loss:  0.49241185\n",
      "Epoch:  5\n",
      "Loss:  0.42767128\n",
      "Epoch:  5\n",
      "Loss:  0.46226043\n",
      "Epoch:  5\n",
      "Loss:  0.48291788\n",
      "Epoch:  5\n",
      "Loss:  0.49959332\n",
      "Epoch:  5\n",
      "Loss:  0.5814648\n",
      "Epoch:  5\n",
      "Loss:  0.52139246\n",
      "Epoch:  5\n",
      "Loss:  0.47851038\n",
      "Epoch:  5\n",
      "Loss:  0.44527712\n",
      "Epoch:  5\n",
      "Loss:  0.57447743\n",
      "Epoch:  5\n",
      "Loss:  0.48865056\n",
      "Epoch:  5\n",
      "Loss:  0.5343769\n",
      "Epoch:  5\n",
      "Loss:  0.56088454\n",
      "Epoch:  5\n",
      "Loss:  0.57110417\n",
      "Epoch:  5\n",
      "Loss:  0.5753442\n",
      "Epoch:  5\n",
      "Loss:  0.49958268\n",
      "Epoch:  5\n",
      "Loss:  0.43109018\n",
      "Epoch:  5\n",
      "Loss:  0.5643545\n",
      "Epoch:  5\n",
      "Loss:  0.5235849\n",
      "Epoch:  5\n",
      "Loss:  0.4579211\n",
      "Epoch:  5\n",
      "Loss:  0.47010684\n",
      "Epoch:  5\n",
      "Loss:  0.5097736\n",
      "Epoch:  5\n",
      "Loss:  0.49356523\n",
      "Epoch:  5\n",
      "Loss:  0.6081467\n",
      "Epoch:  5\n",
      "Loss:  0.5463575\n",
      "Epoch:  5\n",
      "Loss:  0.5501005\n",
      "Epoch:  5\n",
      "Loss:  0.5383681\n",
      "Epoch:  5\n",
      "Loss:  0.43181044\n",
      "Epoch:  5\n",
      "Loss:  0.4470411\n",
      "Epoch:  5\n",
      "Loss:  0.50579315\n",
      "Epoch:  5\n",
      "Loss:  0.48636705\n",
      "Epoch:  5\n",
      "Loss:  0.5046419\n",
      "Epoch:  5\n",
      "Loss:  0.58347356\n",
      "Epoch:  5\n",
      "Loss:  0.4534885\n",
      "Epoch:  5\n",
      "Loss:  0.549576\n",
      "Epoch:  5\n",
      "Loss:  0.4565741\n",
      "Epoch:  5\n",
      "Loss:  0.47959232\n",
      "Epoch:  5\n",
      "Loss:  0.43766755\n",
      "Epoch:  5\n",
      "Loss:  0.4585894\n",
      "Epoch:  5\n",
      "Loss:  0.5268639\n",
      "Epoch:  5\n",
      "Loss:  0.52735966\n",
      "Epoch:  5\n",
      "Loss:  0.4405834\n",
      "Epoch:  5\n",
      "Loss:  0.48532158\n",
      "Epoch:  5\n",
      "Loss:  0.47081727\n",
      "Epoch:  5\n",
      "Loss:  0.5146902\n",
      "Epoch:  6\n",
      "Loss:  0.37242347\n",
      "Epoch:  6\n",
      "Loss:  0.28013653\n",
      "Epoch:  6\n",
      "Loss:  0.3171303\n",
      "Epoch:  6\n",
      "Loss:  0.29253972\n",
      "Epoch:  6\n",
      "Loss:  0.29060572\n",
      "Epoch:  6\n",
      "Loss:  0.30980235\n",
      "Epoch:  6\n",
      "Loss:  0.40694985\n",
      "Epoch:  6\n",
      "Loss:  0.35193765\n",
      "Epoch:  6\n",
      "Loss:  0.26084805\n",
      "Epoch:  6\n",
      "Loss:  0.27574083\n",
      "Epoch:  6\n",
      "Loss:  0.35104063\n",
      "Epoch:  6\n",
      "Loss:  0.2807484\n",
      "Epoch:  6\n",
      "Loss:  0.36588806\n",
      "Epoch:  6\n",
      "Loss:  0.3371355\n",
      "Epoch:  6\n",
      "Loss:  0.29587907\n",
      "Epoch:  6\n",
      "Loss:  0.3115383\n",
      "Epoch:  6\n",
      "Loss:  0.37456718\n",
      "Epoch:  6\n",
      "Loss:  0.33229274\n",
      "Epoch:  6\n",
      "Loss:  0.32995436\n",
      "Epoch:  6\n",
      "Loss:  0.30324176\n",
      "Epoch:  6\n",
      "Loss:  0.22854164\n",
      "Epoch:  6\n",
      "Loss:  0.3503421\n",
      "Epoch:  6\n",
      "Loss:  0.2564362\n",
      "Epoch:  6\n",
      "Loss:  0.33028874\n",
      "Epoch:  6\n",
      "Loss:  0.2769392\n",
      "Epoch:  6\n",
      "Loss:  0.30069992\n",
      "Epoch:  6\n",
      "Loss:  0.3104284\n",
      "Epoch:  6\n",
      "Loss:  0.32966143\n",
      "Epoch:  6\n",
      "Loss:  0.29509524\n",
      "Epoch:  6\n",
      "Loss:  0.31979\n",
      "Epoch:  6\n",
      "Loss:  0.37055153\n",
      "Epoch:  6\n",
      "Loss:  0.3048668\n",
      "Epoch:  6\n",
      "Loss:  0.35088903\n",
      "Epoch:  6\n",
      "Loss:  0.34704667\n",
      "Epoch:  6\n",
      "Loss:  0.31652993\n",
      "Epoch:  6\n",
      "Loss:  0.35562858\n",
      "Epoch:  6\n",
      "Loss:  0.2743804\n",
      "Epoch:  6\n",
      "Loss:  0.30289423\n",
      "Epoch:  6\n",
      "Loss:  0.3044032\n",
      "Epoch:  6\n",
      "Loss:  0.30798414\n",
      "Epoch:  6\n",
      "Loss:  0.29645452\n",
      "Epoch:  6\n",
      "Loss:  0.30572918\n",
      "Epoch:  6\n",
      "Loss:  0.31615517\n",
      "Epoch:  6\n",
      "Loss:  0.31916296\n",
      "Epoch:  6\n",
      "Loss:  0.30436593\n",
      "Epoch:  6\n",
      "Loss:  0.3109785\n",
      "Epoch:  6\n",
      "Loss:  0.29438806\n",
      "Epoch:  6\n",
      "Loss:  0.29509544\n",
      "Epoch:  6\n",
      "Loss:  0.3242084\n",
      "Epoch:  6\n",
      "Loss:  0.3364419\n",
      "Epoch:  6\n",
      "Loss:  0.3251781\n",
      "Epoch:  6\n",
      "Loss:  0.38226894\n",
      "Epoch:  6\n",
      "Loss:  0.3619976\n",
      "Epoch:  6\n",
      "Loss:  0.3749868\n",
      "Epoch:  6\n",
      "Loss:  0.29832026\n",
      "Epoch:  6\n",
      "Loss:  0.33498555\n",
      "Epoch:  6\n",
      "Loss:  0.32819557\n",
      "Epoch:  6\n",
      "Loss:  0.2729289\n",
      "Epoch:  6\n",
      "Loss:  0.3345905\n",
      "Epoch:  6\n",
      "Loss:  0.32312414\n",
      "Epoch:  6\n",
      "Loss:  0.33370867\n",
      "Epoch:  6\n",
      "Loss:  0.3179012\n",
      "Epoch:  6\n",
      "Loss:  0.2476807\n",
      "Epoch:  6\n",
      "Loss:  0.33715403\n",
      "Epoch:  6\n",
      "Loss:  0.31716794\n",
      "Epoch:  6\n",
      "Loss:  0.30060658\n",
      "Epoch:  6\n",
      "Loss:  0.3779124\n",
      "Epoch:  6\n",
      "Loss:  0.29797992\n",
      "Epoch:  6\n",
      "Loss:  0.379736\n",
      "Epoch:  6\n",
      "Loss:  0.32583502\n",
      "Epoch:  6\n",
      "Loss:  0.353648\n",
      "Epoch:  6\n",
      "Loss:  0.27489054\n",
      "Epoch:  6\n",
      "Loss:  0.26257437\n",
      "Epoch:  6\n",
      "Loss:  0.35524553\n",
      "Epoch:  6\n",
      "Loss:  0.2944996\n",
      "Epoch:  6\n",
      "Loss:  0.28546092\n",
      "Epoch:  6\n",
      "Loss:  0.3174879\n",
      "Epoch:  6\n",
      "Loss:  0.341864\n",
      "Epoch:  6\n",
      "Loss:  0.25090334\n",
      "Epoch:  6\n",
      "Loss:  0.3806797\n",
      "Epoch:  6\n",
      "Loss:  0.32262534\n",
      "Epoch:  6\n",
      "Loss:  0.3859318\n",
      "Epoch:  6\n",
      "Loss:  0.3665044\n",
      "Epoch:  6\n",
      "Loss:  0.30610076\n",
      "Epoch:  6\n",
      "Loss:  0.29543772\n",
      "Epoch:  6\n",
      "Loss:  0.29596835\n",
      "Epoch:  6\n",
      "Loss:  0.26915023\n",
      "Epoch:  6\n",
      "Loss:  0.29508403\n",
      "Epoch:  6\n",
      "Loss:  0.3904609\n",
      "Epoch:  6\n",
      "Loss:  0.3222621\n",
      "Epoch:  6\n",
      "Loss:  0.32274115\n",
      "Epoch:  6\n",
      "Loss:  0.31905645\n",
      "Epoch:  6\n",
      "Loss:  0.35946745\n",
      "Epoch:  6\n",
      "Loss:  0.34692827\n",
      "Epoch:  6\n",
      "Loss:  0.35019064\n",
      "Epoch:  6\n",
      "Loss:  0.31360617\n",
      "Epoch:  6\n",
      "Loss:  0.3094016\n",
      "Epoch:  6\n",
      "Loss:  0.29713774\n",
      "Epoch:  6\n",
      "Loss:  0.36799306\n",
      "Epoch:  6\n",
      "Loss:  0.35518733\n",
      "Epoch:  6\n",
      "Loss:  0.3777931\n",
      "Epoch:  6\n",
      "Loss:  0.36246735\n",
      "Epoch:  6\n",
      "Loss:  0.36630642\n",
      "Epoch:  6\n",
      "Loss:  0.31170133\n",
      "Epoch:  6\n",
      "Loss:  0.39623177\n",
      "Epoch:  6\n",
      "Loss:  0.32638717\n",
      "Epoch:  6\n",
      "Loss:  0.32393557\n",
      "Epoch:  6\n",
      "Loss:  0.32821625\n",
      "Epoch:  6\n",
      "Loss:  0.296003\n",
      "Epoch:  6\n",
      "Loss:  0.24624662\n",
      "Epoch:  6\n",
      "Loss:  0.3666287\n",
      "Epoch:  6\n",
      "Loss:  0.27854645\n",
      "Epoch:  6\n",
      "Loss:  0.33604285\n",
      "Epoch:  6\n",
      "Loss:  0.3470372\n",
      "Epoch:  6\n",
      "Loss:  0.35653183\n",
      "Epoch:  6\n",
      "Loss:  0.3068116\n",
      "Epoch:  6\n",
      "Loss:  0.2789677\n",
      "Epoch:  6\n",
      "Loss:  0.34707415\n",
      "Epoch:  6\n",
      "Loss:  0.37066576\n",
      "Epoch:  6\n",
      "Loss:  0.39213032\n",
      "Epoch:  6\n",
      "Loss:  0.36516565\n",
      "Epoch:  6\n",
      "Loss:  0.3415877\n",
      "Epoch:  6\n",
      "Loss:  0.32508418\n",
      "Epoch:  6\n",
      "Loss:  0.38964072\n",
      "Epoch:  6\n",
      "Loss:  0.32996303\n",
      "Epoch:  6\n",
      "Loss:  0.37668732\n",
      "Epoch:  6\n",
      "Loss:  0.36521658\n",
      "Epoch:  6\n",
      "Loss:  0.31378904\n",
      "Epoch:  6\n",
      "Loss:  0.37243634\n",
      "Epoch:  6\n",
      "Loss:  0.31862938\n",
      "Epoch:  6\n",
      "Loss:  0.3369338\n",
      "Epoch:  6\n",
      "Loss:  0.45987892\n",
      "Epoch:  6\n",
      "Loss:  0.2946205\n",
      "Epoch:  6\n",
      "Loss:  0.3281591\n",
      "Epoch:  6\n",
      "Loss:  0.27696925\n",
      "Epoch:  6\n",
      "Loss:  0.36619183\n",
      "Epoch:  6\n",
      "Loss:  0.33573362\n",
      "Epoch:  6\n",
      "Loss:  0.33661544\n",
      "Epoch:  6\n",
      "Loss:  0.39965957\n",
      "Epoch:  6\n",
      "Loss:  0.36972785\n",
      "Epoch:  6\n",
      "Loss:  0.29156747\n",
      "Epoch:  6\n",
      "Loss:  0.39271075\n",
      "Epoch:  6\n",
      "Loss:  0.3239805\n",
      "Epoch:  6\n",
      "Loss:  0.29306307\n",
      "Epoch:  6\n",
      "Loss:  0.33498776\n",
      "Epoch:  6\n",
      "Loss:  0.36285552\n",
      "Epoch:  6\n",
      "Loss:  0.37094802\n",
      "Epoch:  6\n",
      "Loss:  0.33935323\n",
      "Epoch:  6\n",
      "Loss:  0.30411875\n",
      "Epoch:  6\n",
      "Loss:  0.38237026\n",
      "Epoch:  6\n",
      "Loss:  0.3348534\n",
      "Epoch:  6\n",
      "Loss:  0.34650478\n",
      "Epoch:  6\n",
      "Loss:  0.36941156\n",
      "Epoch:  6\n",
      "Loss:  0.37291166\n",
      "Epoch:  6\n",
      "Loss:  0.35124812\n",
      "Epoch:  6\n",
      "Loss:  0.4070363\n",
      "Epoch:  6\n",
      "Loss:  0.30512157\n",
      "Epoch:  6\n",
      "Loss:  0.29738733\n",
      "Epoch:  6\n",
      "Loss:  0.3423671\n",
      "Epoch:  6\n",
      "Loss:  0.3253739\n",
      "Epoch:  6\n",
      "Loss:  0.29695493\n",
      "Epoch:  6\n",
      "Loss:  0.32503724\n",
      "Epoch:  6\n",
      "Loss:  0.34296677\n",
      "Epoch:  6\n",
      "Loss:  0.32422367\n",
      "Epoch:  6\n",
      "Loss:  0.28516468\n",
      "Epoch:  6\n",
      "Loss:  0.28215873\n",
      "Epoch:  6\n",
      "Loss:  0.34726822\n",
      "Epoch:  6\n",
      "Loss:  0.326806\n",
      "Epoch:  6\n",
      "Loss:  0.2896903\n",
      "Epoch:  6\n",
      "Loss:  0.401579\n",
      "Epoch:  6\n",
      "Loss:  0.34321955\n",
      "Epoch:  6\n",
      "Loss:  0.2812989\n",
      "Epoch:  6\n",
      "Loss:  0.32336867\n",
      "Epoch:  6\n",
      "Loss:  0.37979797\n",
      "Epoch:  6\n",
      "Loss:  0.33578923\n",
      "Epoch:  6\n",
      "Loss:  0.33812723\n",
      "Epoch:  6\n",
      "Loss:  0.36205307\n",
      "Epoch:  6\n",
      "Loss:  0.36172998\n",
      "Epoch:  6\n",
      "Loss:  0.34699872\n",
      "Epoch:  6\n",
      "Loss:  0.33663824\n",
      "Epoch:  6\n",
      "Loss:  0.30032557\n",
      "Epoch:  6\n",
      "Loss:  0.35681468\n",
      "Epoch:  6\n",
      "Loss:  0.35344744\n",
      "Epoch:  6\n",
      "Loss:  0.32676333\n",
      "Epoch:  6\n",
      "Loss:  0.3838981\n",
      "Epoch:  6\n",
      "Loss:  0.3092828\n",
      "Epoch:  6\n",
      "Loss:  0.32121044\n",
      "Epoch:  6\n",
      "Loss:  0.36508697\n",
      "Epoch:  6\n",
      "Loss:  0.37406892\n",
      "Epoch:  6\n",
      "Loss:  0.26272586\n",
      "Epoch:  6\n",
      "Loss:  0.35021055\n",
      "Epoch:  6\n",
      "Loss:  0.31647092\n",
      "Epoch:  6\n",
      "Loss:  0.29016596\n",
      "Epoch:  6\n",
      "Loss:  0.2690944\n",
      "Epoch:  6\n",
      "Loss:  0.3540539\n",
      "Epoch:  6\n",
      "Loss:  0.3165796\n",
      "Epoch:  6\n",
      "Loss:  0.40264654\n",
      "Epoch:  6\n",
      "Loss:  0.32940048\n",
      "Epoch:  6\n",
      "Loss:  0.31428802\n",
      "Epoch:  6\n",
      "Loss:  0.35870782\n",
      "Epoch:  6\n",
      "Loss:  0.30864808\n",
      "Epoch:  6\n",
      "Loss:  0.33130735\n",
      "Epoch:  6\n",
      "Loss:  0.3059296\n",
      "Epoch:  6\n",
      "Loss:  0.28131312\n",
      "Epoch:  6\n",
      "Loss:  0.37579277\n",
      "Epoch:  6\n",
      "Loss:  0.3645154\n",
      "Epoch:  6\n",
      "Loss:  0.35131538\n",
      "Epoch:  6\n",
      "Loss:  0.31450522\n",
      "Epoch:  6\n",
      "Loss:  0.36001962\n",
      "Epoch:  6\n",
      "Loss:  0.31362176\n",
      "Epoch:  6\n",
      "Loss:  0.32588625\n",
      "Epoch:  6\n",
      "Loss:  0.43281013\n",
      "Epoch:  6\n",
      "Loss:  0.32078117\n",
      "Epoch:  6\n",
      "Loss:  0.28129083\n",
      "Epoch:  6\n",
      "Loss:  0.31565446\n",
      "Epoch:  6\n",
      "Loss:  0.32744855\n",
      "Epoch:  6\n",
      "Loss:  0.3698234\n",
      "Epoch:  6\n",
      "Loss:  0.42736608\n",
      "Epoch:  6\n",
      "Loss:  0.31911594\n",
      "Epoch:  6\n",
      "Loss:  0.406876\n",
      "Epoch:  6\n",
      "Loss:  0.32885864\n",
      "Epoch:  6\n",
      "Loss:  0.39366335\n",
      "Epoch:  6\n",
      "Loss:  0.37274\n",
      "Epoch:  6\n",
      "Loss:  0.36603308\n",
      "Epoch:  6\n",
      "Loss:  0.38276488\n",
      "Epoch:  6\n",
      "Loss:  0.39988625\n",
      "Epoch:  6\n",
      "Loss:  0.32780322\n",
      "Epoch:  6\n",
      "Loss:  0.3606528\n",
      "Epoch:  6\n",
      "Loss:  0.3607504\n",
      "Epoch:  6\n",
      "Loss:  0.36657107\n",
      "Epoch:  6\n",
      "Loss:  0.39910418\n",
      "Epoch:  6\n",
      "Loss:  0.3516432\n",
      "Epoch:  6\n",
      "Loss:  0.41590095\n",
      "Epoch:  6\n",
      "Loss:  0.34595996\n",
      "Epoch:  6\n",
      "Loss:  0.31648064\n",
      "Epoch:  6\n",
      "Loss:  0.3555854\n",
      "Epoch:  6\n",
      "Loss:  0.33474207\n",
      "Epoch:  6\n",
      "Loss:  0.33249068\n",
      "Epoch:  6\n",
      "Loss:  0.32796162\n",
      "Epoch:  6\n",
      "Loss:  0.31269702\n",
      "Epoch:  6\n",
      "Loss:  0.33474857\n",
      "Epoch:  6\n",
      "Loss:  0.36519167\n",
      "Epoch:  6\n",
      "Loss:  0.3637972\n",
      "Epoch:  6\n",
      "Loss:  0.39622006\n",
      "Epoch:  6\n",
      "Loss:  0.3862278\n",
      "Epoch:  6\n",
      "Loss:  0.38073424\n",
      "Epoch:  6\n",
      "Loss:  0.354209\n",
      "Epoch:  6\n",
      "Loss:  0.370614\n",
      "Epoch:  6\n",
      "Loss:  0.37738588\n",
      "Epoch:  6\n",
      "Loss:  0.3297319\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 7\n",
    "for epoch in range(EPOCH):\n",
    "    enc_hidden = encoder.initilize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        print('Epoch: ', epoch)\n",
    "        print('Loss: ', batch_loss.numpy())\n",
    "    checkpoint.save(file_prefix='chp/test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess_senetence(sentence)\n",
    "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<strat>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x21556ebf8c0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint('chp/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it s very hot . <end> ', '<strat> hace mucho frio aqui . <end>')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('hace mucho frio aqui.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
